Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Assessment2010,
abstract = {Ce rapport fournit des estimations des gaz {\`{a}} effet de serre associ{\'{e}}s {\`{a}} la prodction et la transformation du lait pour certaines r{\'{e}}gions et syst{\`{e}}mes agricoles du monde. Orient{\'{e}} {\'{e}}missions de gaz {\`{a}} effet de serre uniquement},
author = {Assessment, a Life Cycle},
doi = {10.1016/S0301-4215(01)00105-7},
issn = {03014215},
journal = {Africa},
title = {{Greenhouse Gas Emissions from the Dairy Sector}},
year = {2010}
}
@article{Chow2014a,
abstract = {In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in costs in addition to minimizing a standard criterion. Conditional value-at-risk (CVaR) is a relatively new risk measure that addresses some of the shortcomings of the well-known variance-related risk measures, and because of its computational efficiencies has gained popularity in finance and operations research. In this paper, we consider the mean-CVaR optimization problem in MDPs. We first derive a formula for computing the gradient of this risk-sensitive objective function. We then devise policy gradient and actor-critic algorithms that each uses a specific method to estimate this gradient and updates the policy parameters in the descent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in an optimal stopping problem.},
archivePrefix = {arXiv},
arxivId = {1406.3339},
author = {Chow, Yinlam and Ghavamzadeh, Mohammad},
eprint = {1406.3339},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/Extended{\_}1Chow{\_}algorithms-for-cvar-optimization-in-mdps.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {January},
pages = {3509--3517},
title = {{Algorithms for CVaR optimization in MDPs}},
volume = {4},
year = {2014}
}
@inproceedings{Dabney2018,
abstract = {In reinforcement learning (RL), an agent interacts with the environment by taking actions and observing the next state and reward. When sampled probabilistically, these state transitions, rewards, and actions can all induce randomness in the observed long-term return. Traditionally, reinforcement learning algorithms average over this randomness to estimate the value function. In this paper, we build on recent work advocating a distributional approach to reinforcement learning in which the distribution over returns is modeled explicitly instead of only estimating the mean. That is, we examine methods of learning the value distribution instead of the value function. We give results that close a number of gaps between the theoretical and algorithmic results given by Bellemare, Dabney, and Munos (2017). First, we extend existing results to the approximate distribution setting. Second, we present a novel distributional reinforcement learning algorithm consistent with our theoretical formulation. Finally, we evaluate this new algorithm on the Atari 2600 games, observing that it significantly outperforms many of the recent improvements on DQN, including the related distributional algorithm C51.},
archivePrefix = {arXiv},
arxivId = {1710.10044},
author = {Dabney, Will and Rowland, Mark and Bellemare, Marc G. and Munos, R{\'{e}}mi},
booktitle = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
eprint = {1710.10044},
isbn = {9781577358008},
title = {{Distributional reinforcement learning with quantile regression}},
year = {2018}
}
@article{Curi2019,
abstract = {We consider the problem of training machine learning models in a risk-averse manner. In particular, we propose an adaptive sampling algorithm for stochastically optimizing the Conditional Value-at-Risk (CVaR) of a loss distribution. We use a distributionally robust formulation of the CVaR to phrase the problem as a zero-sum game between two players. Our approach solves the game using an efficient no-regret algorithm for each player. Critically, we can apply these algorithms to large-scale settings because the implementation relies on sampling from Determinantal Point Processes. Finally, we empirically demonstrate its effectiveness on large-scale convex and non-convex learning tasks.},
archivePrefix = {arXiv},
arxivId = {1910.12511},
author = {Curi, Sebastian and Levy, Kfir. Y. and Jegelka, Stefanie and Krause, Andreas},
eprint = {1910.12511},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/AdaptiveSampling.pdf:pdf},
title = {{Adaptive Sampling for Stochastic Risk-Averse Learning}},
url = {http://arxiv.org/abs/1910.12511},
year = {2019}
}
@inproceedings{Pratt1995SEA,
author = {Pratt, Gill A and Williamson, Matthew M},
booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.1995.525827},
pages = {3137--3181},
title = {{Series elastic actuators}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=525827},
year = {1995}
}
@inproceedings{Bellemare2017,
abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
archivePrefix = {arXiv},
arxivId = {1707.06887},
author = {Bellemare, Marc G. and Dabney, Will and Munos, R{\'{e}}mi},
booktitle = {34th International Conference on Machine Learning, ICML 2017},
eprint = {1707.06887},
isbn = {9781510855144},
title = {{A distributional perspective on reinforcement learning}},
year = {2017}
}
@incollection{Serraino2013,
author = {Serraino, Gaia and Uryasev, Stanislav},
booktitle = {Encyclopedia of Operations Research and Management Science},
doi = {10.1007/978-1-4419-1153-7_1232},
title = {{Conditional Value-at-Risk (CVaR)}},
year = {2013}
}
@article{Artzner1999,
abstract = {In this paper we study both market risks and nonmarket risks, without complete markets assumption, and discuss methods of measurement of these risks. We present and justify a set of four desirable properties for measures of risk, and call the measures satisfying these properties "coherent." We examine the measures of risk provided and the related actions required by SPAN, by the SEC/NASD rules, and by quantile-based methods. We demonstrate the universality of scenario-based methods for providing coherent measures. We offer suggestions concerning the SEC method. We also suggest a method to repair the failure of subadditivity of quantile-based methods.},
author = {Artzner, Philippe and Delbaen, Freddy and Eber, Jean Marc and Heath, David},
doi = {10.1111/1467-9965.00068},
issn = {09601627},
journal = {Mathematical Finance},
keywords = {Aggregation of risks,Butterfly,Capital requirement,Coherent risk measure,Concentration of risks,Currency risk,Decentralization,Extremal events risk,Insurance risk,Margin requirement,Market risk,Mean excess function,Measure of risk,Model risk},
title = {{Coherent measures of risk}},
year = {1999}
}
@article{Rockafellar2000,
abstract = {A new approach to optimizing or hedging a portfolio of financial instruments to reduce risk is presented and tested on applications. It focuses on minimizing conditional value-at-risk (CVaR) rather than minimizing value-at-risk (VaR), but portfolios with low CVaR necessarily have low VaR as well. CVaR, also called mean excess loss, mean shortfall, or tail VaR, is in any case considered to be a more consistent measure of risk than VaR. Central to the new approach is a technique for portfolio optimization which calculates VaR and optimizes CVaR simultaneously. This technique is suitable for use by investment companies, brokerage firms, mutual funds, and any business that evaluates risk. It can be combined with analytical or scenario-based methods to optimize portfolios with large numbers of instruments, in which case the calculations often come down to linear programming or nonsmooth programming. The methodology can also be applied to the optimization of percentiles in contexts outside of finance.},
author = {Rockafellar, R. Tyrrell and Uryasev, Stanislav},
doi = {10.21314/jor.2000.038},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/important{\_}cvar{\_}read/2CVAR.pdf:pdf},
issn = {14651211},
journal = {The Journal of Risk},
number = {3},
pages = {21--41},
title = {{Optimization of conditional value-at-risk}},
volume = {2},
year = {2000}
}
@article{Rockafellar2002,
abstract = {Fundamental properties of conditional value-at-risk (CVaR), as a measure of risk with significant advantages over value-at-risk (VaR), are derived for loss distributions in finance that can involve discreetness. Such distributions are of particular importance in applications because of the prevalence of models based on scenarios and finite sampling. CVaR is able to quantify dangers beyond VaR and moreover it is coherent. It provides optimization short-cuts which, through linear programming techniques, make practical many large-scale calculations that could otherwise be out of reach. The numerical efficiency and stability of such calculations, shown in several case studies, are illustrated further with an example of index tracking. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
author = {Rockafellar, R. Tyrrell and Uryasev, Stanislav},
doi = {10.1016/S0378-4266(02)00271-6},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/Rockafellar{\_}cvar{\_}for{\_}general{\_}loss{\_}distrib.pdf:pdf},
issn = {03784266},
journal = {Journal of Banking and Finance},
keywords = {Coherent risk measures,Conditional value-at-risk,Hedging,Index tracking,Mean shortfall,Portfolio optimization,Risk management,Risk sampling,Scenarios,Value-at-risk},
number = {7},
pages = {1443--1471},
title = {{Conditional value-at-risk for general loss distributions}},
volume = {26},
year = {2002}
}
@article{Tamar2015a,
abstract = {Conditional Value at Risk (CVaR) is a prominent risk measure that is being used extensively in various domains. We develop a new formula for the gradient of the CVaR in the form of a conditional expectation. Based on this formula, we propose a novel sampling-based estimator for the gradient of the CVaR, in the spirit of the likelihood-ratio method. We analyze the bias of the estimator, and prove the convergence of a corresponding stochastic gradient descent algorithm to a local CVaR optimum. Our method allows to consider CVaR optimization in new domains. As an example, we consider a reinforcement learning application, and learn a risksensitive controller for the game of Tetris.},
archivePrefix = {arXiv},
arxivId = {1404.3862},
author = {Tamar, Aviv and Glassner, Yonatan and Mannor, Shie},
eprint = {1404.3862},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/new{\_}from{\_}friday{\_}9429-45152-1-PB.pdf:pdf},
isbn = {9781577357025},
journal = {Proceedings of the National Conference on Artificial Intelligence},
keywords = {Novel Machine Learning Algorithms Track},
pages = {2993--2999},
title = {{Optimizing the CVaR via sampling}},
volume = {4},
year = {2015}
}
@article{Ruszczynski2010,
abstract = {We introduce the concept of a Markov risk measure and we use it to formulate risk-averse control problems for two Markov decision models: a finite horizon model and a discounted infinite horizon model. For both models we derive risk-averse dynamic programming equations and a value iteration method. For the infinite horizon problem we develop a risk-averse policy iteration method and we prove its convergence. We also propose a version of the Newton method to solve a nonsmooth equation arising in the policy iteration method and we prove its global convergence. Finally, we discuss relations to min-max Markov decision models. {\textcopyright} 2010 Springer and Mathematical Optimization Society.},
author = {Ruszczy{\'{n}}ski, Andrzej},
doi = {10.1007/s10107-010-0393-3},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/Ruszczy{\'{n}}ski2010{\_}Article{\_}Risk-averseDynamicProgrammingF.pdf:pdf},
isbn = {1010701003933},
issn = {14364646},
journal = {Mathematical Programming},
keywords = {Dynamic risk measures,Markov risk measures,Min-max Markov models,Nonsmooth Newton's method,Policy iteration,Value iteration},
number = {2},
pages = {235--261},
title = {{Risk-averse dynamic programming for Markov decision processes}},
volume = {125},
year = {2010}
}
@article{Tamar2015,
abstract = {Several authors have recently developed risk-sensitive policy gradient methods that augment the standard expected cost minimization problem with a measure of variability in cost. These studies have focused on specific risk-measures, such as the variance or conditional value at risk (CVaR). In this work, we extend the policy gradient method to the whole class of coherent risk measures, which is widely accepted in finance and operations research, among other fields. We consider both static and time-consistent dynamic risk measures. For static risk measures, our approach is in the spirit of policy gradient algorithms and combines a standard sampling approach with convex programming. For dynamic risk measures, our approach is actor-critic style and involves explicit approximation of value function. Most importantly, our contribution presents a unified approach to risk-sensitive reinforcement learning that generalizes and extends previous results.},
archivePrefix = {arXiv},
arxivId = {1502.03919},
author = {Tamar, Aviv and Chow, Yinlam and Ghavamzadeh, Mohammad and Mannor, Shie},
eprint = {1502.03919},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/Policy-gradient-for-coherent-risk-measures.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1468--1476},
title = {{Policy gradient for coherent risk measures}},
volume = {2015-Janua},
year = {2015}
}
@article{Tamar2017,
abstract = {We provide sampling-based algorithms for optimization under a coherent-risk objective. The class of coherent-risk measures is widely accepted in finance and operations research, among other fields, and encompasses popular risk-measures such as conditional value at risk and mean-semi-deviation. Our approach is suitable for problems in which tuneable parameters control the distribution of the cost, such as in reinforcement learning or approximate dynamic programming with a parameterized policy. Such problems cannot be solved using previous approaches. We consider both static risk measures and time-consistent dynamic risk measures. For static risk measures, our approach is in the spirit of policy gradient methods, while for the dynamic risk measures, we use actor-critic type algorithms.},
author = {Tamar, Aviv and Chow, Yinlam and Ghavamzadeh, Mohammad and Mannor, Shie},
doi = {10.1109/TAC.2016.2644871},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/Sequential Decision Making With Coherent Risk.pdf:pdf},
issn = {00189286},
journal = {IEEE Transactions on Automatic Control},
keywords = {Coherent risk,Markov decision processes,dynamic programming,policy gradient},
number = {7},
pages = {3323--3338},
publisher = {IEEE},
title = {{Sequential Decision Making with Coherent Risk}},
volume = {62},
year = {2017}
}
@article{Science2014,
author = {Science, Source Management and Mar, Theory Series and Howardt, Ronald A and Mathesont, James E},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/Risk-Sensitive Markov Decision Process{\_}Howardetal.pdf:pdf},
number = {7},
pages = {356--369},
title = {{Risk-Sensitive Markov Decision Processes Author ( s ): Ronald A . Howard and James E . Matheson Published by : INFORMS Stable URL : http://www.jstor.org/stable/2629352 . RISK-SENSITIVE MARKOV DECISION PROCESSES *}},
volume = {18},
year = {2014}
}
@article{Chow2014,
abstract = {In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in costs in addition to minimizing a standard criterion. Conditional value-at-risk (CVaR) is a relatively new risk measure that addresses some of the shortcomings of the well-known variance-related risk measures, and because of its computational efficiencies has gained popularity in finance and operations research. In this paper, we consider the mean-CVaR optimization problem in MDPs. We first derive a formula for computing the gradient of this risk-sensitive objective function. We then devise policy gradient and actor-critic algorithms that each uses a specific method to estimate this gradient and updates the policy parameters in the descent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in an optimal stopping problem.},
archivePrefix = {arXiv},
arxivId = {1406.3339},
author = {Chow, Yinlam and Ghavamzadeh, Mohammad},
eprint = {1406.3339},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/1Chow{\_}algorithms-for-cvar-optimization-in-mdps.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {January},
pages = {3509--3517},
title = {{Algorithms for CVaR optimization in MDPs}},
volume = {4},
year = {2014}
}
@article{Vukobratovic2004ZeroMomentPoint,
author = {Vukobratovi{\'{c}}, Miomir and Borovac, Branislav},
issn = {0219-8436},
journal = {International Journal of Humanoid Robotics},
number = {01},
pages = {157--173},
publisher = {World Scientific},
title = {{Zero-moment point — thirty five years of its life}},
volume = {1},
year = {2004}
}
@article{Sutton1998,
abstract = {CiteSeerX - Scientific documents that cite the following paper: Reinforcement learning: An introduction, chapter 11},
author = {Sutton, R.S. and Barto, A.G.},
doi = {10.1109/tnn.1998.712192},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
title = {{Reinforcement Learning: An Introduction}},
year = {1998}
}
@article{Carpin2016,
abstract = {In this paper we present an algorithm to compute risk averse policies in Markov Decision Processes (MDP) when the total cost criterion is used together with the average value at risk (AVaR) metric. Risk averse policies are needed when large deviations from the expected behavior may have detrimental effects, and conventional MDP algorithms usually ignore this aspect. We provide conditions for the structure of the underlying MDP ensuring that approximations for the exact problem can be derived and solved efficiently. Our findings are novel inasmuch as average value at risk has not previously been considered in association with the total cost criterion. Our method is demonstrated in a rapid deployment scenario, whereby a robot is tasked with the objective of reaching a target location within a temporal deadline where increased speed is associated with increased probability of failure. We demonstrate that the proposed algorithm not only produces a risk averse policy reducing the probability of exceeding the expected temporal deadline, but also provides the statistical distribution of costs, thus offering a valuable analysis tool.},
archivePrefix = {arXiv},
arxivId = {1602.05130},
author = {Carpin, Stefano and Chow, Yin Lam and Pavone, Marco},
doi = {10.1109/ICRA.2016.7487152},
eprint = {1602.05130},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/RISK-AVERSE/Risk aversion in finite markov decision processes using total cost criteria and average value at risK.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {335--342},
title = {{Risk aversion in finite Markov Decision Processes using total cost criteria and average value at risk}},
volume = {2016-June},
year = {2016}
}
@book{Raibert1986LeggedRobotsThatBalance,
address = {Cambridge, MA},
author = {Raibert, Marc},
isbn = {0262181177},
publisher = {MIT Press},
title = {{Legged Robots That Balance}},
url = {http://portal.acm.org/citation.cfm?id=6152{\&}dl=GUIDE},
year = {1986}
}
@article{Marcus1997,
author = {Marcus, Steven I},
doi = {10.1007/978-1-4612-4120-1},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/Risk{\_}Sensitive{\_}Markov{\_}Decision{\_}Processes.pdf:pdf},
isbn = {9781461241201},
journal = {Systems and Control in the Twenty-First Century},
number = {January 1997},
title = {{Systems and Control in the Twenty-First Century}},
year = {1997}
}
@article{Chow2015,
abstract = {In this paper we address the problem of decision making within a Markov decision process (MDP) framework where risk and modeling errors are taken into account. Our approach is to minimize a risk-sensitive conditional-value-at-risk (CVaR) objective, as opposed to a standard risk-neutral expectation. We refer to such problem as CVaR MDP. Our first contribution is to show that a CVaR objective, besides capturing risk sensitivity, has an alternative interpretation as expected cost under worst-case modeling errors, for a given error budget. This result, which is of independent interest, motivates CVaR MDPs as a unifying framework for risk-sensitive and robust decision making. Our second contribution is to present an approximate value-iteration algorithm for CVaR MDPs and analyze its convergence rate. To our knowledge, this is the first solution algorithm for CVaR MDPs that enjoys error guarantees. Finally, we present results from numerical experiments that corroborate our theoretical findings and show the practicality of our approach.},
archivePrefix = {arXiv},
arxivId = {1506.02188},
author = {Chow, Yinlam and Tamar, Aviv and Mannor, Shie and Pavone, Marco},
eprint = {1506.02188},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/3Risk-sensitiveRobustDecisionMaking{\_}cvar.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1522--1530},
title = {{Risk-sensitive and robust decision-making: A CVaR optimization approach}},
volume = {2015-Janua},
year = {2015}
}
