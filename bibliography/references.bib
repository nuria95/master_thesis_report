Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Chung1987,
abstract = {The present value of the rewards associated with a discrete-time Markov process has a probability distribution which depends on the initial state. The first part of the paper applies fixed point theory to a system of equations for the distribution functions of the present value. The second part of the paper expands the model to a Markov decision process (MDP) and considers the maximization of the expected utility of the present value when the utility function is exponential.},
author = {Chung, Kun Jen and Sobel, Matthew J.},
doi = {10.1137/0325004},
issn = {03630129},
journal = {SIAM Journal on Control and Optimization},
title = {{DISCOUNTED MDP'S: DISTRIBUTION FUNCTIONS AND EXPONENTIAL UTILITY MAXIMIZATION.}},
year = {1987}
}
@inproceedings{Tamar2012,
abstract = {Managing risk in dynamic decision problems is of cardinal importance in many fields such as finance and process control. The most common approach to defining risk is through various variance related criteria such as the Sharpe Ratio or the standard deviation adjusted reward. It is known that optimizing many of the variance related risk criteria is NP-hard. In this paper we devise a framework for local policy gradient style algorithms for reinforcement learning for variance related criteria. Our starting point is a new formula for the variance of the cost-to-go in episodic tasks. Using this formula we develop policy gradient algorithms for criteria that involve both the expected cost and the variance of the cost. We prove the convergence of these algorithms to local minima and demonstrate their applicability in a portfolio planning problem. Copyright 2012 by the author(s)/owner(s).},
author = {Tamar, Aviv and {Di Castro}, Dotan and Mannor, Shie},
booktitle = {Proceedings of the 29th International Conference on Machine Learning, ICML 2012},
isbn = {9781450312851},
title = {{Policy gradients with variance related risk criteria}},
year = {2012}
}
@article{Ruszczynski2010,
abstract = {We introduce the concept of a Markov risk measure and we use it to formulate risk-averse control problems for two Markov decision models: a finite horizon model and a discounted infinite horizon model. For both models we derive risk-averse dynamic programming equations and a value iteration method. For the infinite horizon problem we develop a risk-averse policy iteration method and we prove its convergence. We also propose a version of the Newton method to solve a nonsmooth equation arising in the policy iteration method and we prove its global convergence. Finally, we discuss relations to min-max Markov decision models. {\textcopyright} 2010 Springer and Mathematical Optimization Society.},
author = {Ruszczy{\'{n}}ski, Andrzej},
doi = {10.1007/s10107-010-0393-3},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/Ruszczy{\'{n}}ski2010{\_}Article{\_}Risk-averseDynamicProgrammingF.pdf:pdf},
isbn = {1010701003933},
issn = {14364646},
journal = {Mathematical Programming},
keywords = {Dynamic risk measures,Markov risk measures,Min-max Markov models,Nonsmooth Newton's method,Policy iteration,Value iteration},
number = {2},
pages = {235--261},
title = {{Risk-averse dynamic programming for Markov decision processes}},
volume = {125},
year = {2010}
}
@inproceedings{Morimura2010a,
abstract = {Most conventional Reinforcement Learning (RL) algorithms aim to optimize decision-making rules in terms of the expected re-turns. However, especially for risk management purposes, other risk-sensitive criteria such as the value-at-risk or the expected shortfall are sometimes preferred in real applications. Here, we describe a parametric method for estimating density of the returns, which allows us to handle various criteria in a unified manner. We first extend the Bellman equation for the conditional expected return to cover a conditional probability density of the returns. Then we derive an extension of the TD-learning algorithm for estimating the return densities in an unknown environment. As test instances, several parametric density estimation algorithms are presented for the Gaussian, Laplace, and skewed Laplace distributions. We show that these algorithms lead to risk-sensitive as well as robust RL paradigms through numerical experiments.},
archivePrefix = {arXiv},
arxivId = {1203.3497},
author = {Morimura, Tetsuro and Sugiyama, Masashi and Kashima, Hisashi and Hachiya, Hirotaka and Tanaka, Toshiyuki},
booktitle = {Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence, UAI 2010},
eprint = {1203.3497},
isbn = {9780974903965},
title = {{Parametric return density estimation for Reinforcement Learning}},
year = {2010}
}
@inproceedings{Moldovan2012,
abstract = {In environments with uncertain dynamics exploration is necessary to learn how to perform well. Existing reinforcement learning algorithms provide strong exploration guarantees, but they tend to rely on an ergodicity assumption. The essence of ergodicity is that any state is eventually reachable from any other state by following a suitable policy. This assumption allows for exploration algorithms that operate by simply favoring states that have rarely been visited before. For most physical systems this assumption is impractical as the systems would break before any reasonable exploration has taken place, i.e., most physical systems don't satisfy the ergodicity assumption. In this paper we address the need for safe exploration methods in Markov decision processes. We first propose a general formulation of safety through ergodicity. We show that imposing safety by restricting attention to the resulting set of guaranteed safe policies is NP-hard. We then present an efficient algorithm for guaranteed safe, but potentially suboptimal, exploration. At the core is an optimization formulation in which the constraints restrict attention to a subset of the guaranteed safe policies and the objective favors exploration policies. Our framework is compatible with the majority of previously proposed exploration methods, which rely on an exploration bonus. Our experiments, which include a Martian terrain exploration problem, show that our method is able to explore better than classical exploration methods. Copyright 2012 by the author(s)/owner(s).},
archivePrefix = {arXiv},
arxivId = {1205.4810},
author = {Moldovan, Teodor Mihai and Abbeel, Pieter},
booktitle = {Proceedings of the 29th International Conference on Machine Learning, ICML 2012},
eprint = {1205.4810},
isbn = {9781450312851},
title = {{Safe exploration in Markov decision processes}},
year = {2012}
}
@inproceedings{Pratt1995SEA,
author = {Pratt, Gill A and Williamson, Matthew M},
booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.1995.525827},
pages = {3137--3181},
title = {{Series elastic actuators}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=525827},
year = {1995}
}
@article{Tamar2015a,
abstract = {Conditional Value at Risk (CVaR) is a prominent risk measure that is being used extensively in various domains. We develop a new formula for the gradient of the CVaR in the form of a conditional expectation. Based on this formula, we propose a novel sampling-based estimator for the gradient of the CVaR, in the spirit of the likelihood-ratio method. We analyze the bias of the estimator, and prove the convergence of a corresponding stochastic gradient descent algorithm to a local CVaR optimum. Our method allows to consider CVaR optimization in new domains. As an example, we consider a reinforcement learning application, and learn a risksensitive controller for the game of Tetris.},
archivePrefix = {arXiv},
arxivId = {1404.3862},
author = {Tamar, Aviv and Glassner, Yonatan and Mannor, Shie},
eprint = {1404.3862},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/new{\_}from{\_}friday{\_}9429-45152-1-PB.pdf:pdf},
isbn = {9781577357025},
journal = {Proceedings of the National Conference on Artificial Intelligence},
keywords = {Novel Machine Learning Algorithms Track},
pages = {2993--2999},
title = {{Optimizing the CVaR via sampling}},
volume = {4},
year = {2015}
}
@article{Koenker2001,
author = {Koenker, Roger and Hallock, Kevin F.},
doi = {10.1257/jep.15.4.143},
issn = {08953309},
journal = {Journal of Economic Perspectives},
title = {{Quantile regression}},
year = {2001}
}
@inproceedings{Kadota2006,
abstract = {We consider utility-constrained Markov decision processes. The expected utility of the total discounted reward is maximized subject to multiple expected utility constraints. By introducing a corresponding Lagrange function, a saddle-point theorem of the utility constrained optimization is derived. The existence of a constrained optimal policy is characterized by optimal action sets specified with a parametric utility. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
author = {Kadota, Yoshinobu and Kurano, Masami and Yasuda, Masami},
booktitle = {Computers and Mathematics with Applications},
doi = {10.1016/j.camwa.2005.11.013},
issn = {08981221},
keywords = {Constrained optimal policy,Discount criterion,Lagrange technique,Markov decision processes,Saddle-point,Utility constraints},
title = {{Discounted Markov decision processes with utility constraints}},
year = {2006}
}
@article{Chow2014,
abstract = {In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in costs in addition to minimizing a standard criterion. Conditional value-at-risk (CVaR) is a relatively new risk measure that addresses some of the shortcomings of the well-known variance-related risk measures, and because of its computational efficiencies has gained popularity in finance and operations research. In this paper, we consider the mean-CVaR optimization problem in MDPs. We first derive a formula for computing the gradient of this risk-sensitive objective function. We then devise policy gradient and actor-critic algorithms that each uses a specific method to estimate this gradient and updates the policy parameters in the descent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in an optimal stopping problem.},
archivePrefix = {arXiv},
arxivId = {1406.3339},
author = {Chow, Yinlam and Ghavamzadeh, Mohammad},
eprint = {1406.3339},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/1Chow{\_}algorithms-for-cvar-optimization-in-mdps.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {January},
pages = {3509--3517},
title = {{Algorithms for CVaR optimization in MDPs}},
volume = {4},
year = {2014}
}
@inproceedings{Kingma2014,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P. and Welling, Max},
booktitle = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
eprint = {1312.6114},
title = {{Auto-encoding variational bayes}},
year = {2014}
}
@inproceedings{Dabney2018a,
abstract = {In reinforcement learning (RL), an agent interacts with the environment by taking actions and observing the next state and reward. When sampled probabilistically, these state transitions, rewards, and actions can all induce randomness in the observed long-term return. Traditionally, reinforcement learning algorithms average over this randomness to estimate the value function. In this paper, we build on recent work advocating a distributional approach to reinforcement learning in which the distribution over returns is modeled explicitly instead of only estimating the mean. That is, we examine methods of learning the value distribution instead of the value function. We give results that close a number of gaps between the theoretical and algorithmic results given by Bellemare, Dabney, and Munos (2017). First, we extend existing results to the approximate distribution setting. Second, we present a novel distributional reinforcement learning algorithm consistent with our theoretical formulation. Finally, we evaluate this new algorithm on the Atari 2600 games, observing that it significantly outperforms many of the recent improvements on DQN, including the related distributional algorithm C51.},
archivePrefix = {arXiv},
arxivId = {1710.10044},
author = {Dabney, Will and Rowland, Mark and Bellemare, Marc G. and Munos, R{\'{e}}mi},
booktitle = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
eprint = {1710.10044},
isbn = {9781577358008},
title = {{Distributional reinforcement learning with quantile regression}},
year = {2018}
}
@incollection{Serraino2013,
author = {Serraino, Gaia and Uryasev, Stanislav},
booktitle = {Encyclopedia of Operations Research and Management Science},
doi = {10.1007/978-1-4419-1153-7_1232},
title = {{Conditional Value-at-Risk (CVaR)}},
year = {2013}
}
@inproceedings{Degris2012a,
abstract = {This paper presents the first actor-critic algorithm for off-policy reinforcement learning. Our algorithm is online and incremental, and its per-time-step complexity scales linearly with the number of learned weights. Previous work on actor-critic algorithms is limited to the on-policy setting and does not take advantage of the recent advances in off-policy gradient temporal-difference learning. Off-policy techniques, such as Greedy-GQ, enable a target policy to be learned while following and obtaining data from another (behavior) policy. For many problems, how-ever, actor-critic methods are more practical than action value methods (like Greedy-GQ) because they explicitly represent the policy; consequently, the policy can be stochastic and utilize a large action space. In this paper, we illustrate how to practically combine the generality and learning potential of off-policy learning with the flexibility in action selection given by actor-critic methods. We derive an incremental, linear time and space complexity algorithm that includes eligibility traces, prove convergence under assumptions similar to previous off-policy algorithms, and empirically show better or comparable performance to existing algorithms on standard reinforcement-learning benchmark problems. Copyright 2012 by the author(s)/owner(s).},
author = {Degris, Thomas and White, Martha and Sutton, Richard S.},
booktitle = {Proceedings of the 29th International Conference on Machine Learning, ICML 2012},
isbn = {9781450312851},
title = {{Off-policy actor-critic}},
year = {2012}
}
@article{Vukobratovic2004ZeroMomentPoint,
author = {Vukobratovi{\'{c}}, Miomir and Borovac, Branislav},
issn = {0219-8436},
journal = {International Journal of Humanoid Robotics},
number = {01},
pages = {157--173},
publisher = {World Scientific},
title = {{Zero-moment point — thirty five years of its life}},
volume = {1},
year = {2004}
}
@article{Dhaene2012,
abstract = {Distorted expectations can be expressed as weighted averages of quantiles. In this note, we show that this statement is essentially true, but that one has to be careful with the correct formulation of it. Furthermore, the proofs of the additivity property for distorted expectations of a comonotonic sum that appear in the literature often do not cover the case of a general distortion function. We present a straightforward proof for the general case, making use of the appropriate expressions for distorted expectations in terms of quantiles.},
author = {Dhaene, Jan and Kukush, Alexander and Linders, Dani{\"{e}}l and Tang, Qihe},
doi = {10.1007/s13385-012-0058-0},
issn = {21909741},
journal = {European Actuarial Journal},
keywords = {Comonotonicity,Distorted expectation,Distortion risk measure,Quantile,TVaR},
title = {{Remarks on quantiles and distortion risk measures}},
year = {2012}
}
@inproceedings{Morimura2010b,
abstract = {Standard Reinforcement Learning (RL) aims to optimize decision-making rules in terms of the expected return. However, especially for risk-management purposes, other criteria such as the expected shortfall are some-times preferred. Here, we describe a method of approximating the distribution of returns, which allows us to derive various kinds of information about the returns. We first show that the Bellman equation, which is a recursive formula for the expected return, can be extended to the cumulative return distribution. Then we derive a nonparametric return distribution estimator with particle smooth ing based on this extended Bellman equation. A key aspect of the proposed algorithm is to represent the recursion relation in the extended Bellman equation by a simple replacement procedure of particles associated with a state by using those of the successor state. We show that our algorithm leads to a risk-sensitive R.L paradigm. The usefulness of the proposed approach is demonstrated through numerical experiments. Copyright 2010 by the author(s)/owner(s).},
author = {Morimura, Tetsuro and Sugiyama, Masashi and Kashima, Hisashi and Hachiya, Hirotaka and Tanaka, Toshiyuki},
booktitle = {ICML 2010 - Proceedings, 27th International Conference on Machine Learning},
isbn = {9781605589077},
title = {{Nonparametric return distribution approximation for reinforcement learning}},
year = {2010}
}
@inproceedings{Geibel2006,
abstract = {In this article, I will consider Markov Decision Processes with two criteria, each defined as the expected value of an infinite horizon cumulative return. The second criterion is either itself subject to an inequality constraint, or there is maximum allowable probability that the single returns violate the constraint. I describe and discuss three new reinforcement learning approaches for solving such control problems.},
address = {Berlin, Heidelberg},
author = {Geibel, Peter},
booktitle = {Machine Learning: ECML 2006},
editor = {F{\"{u}}rnkranz, Johannes and Scheffer, Tobias and Spiliopoulou, Myra},
isbn = {978-3-540-46056-5},
pages = {646--653},
publisher = {Springer Berlin Heidelberg},
title = {{Reinforcement Learning for MDPs with Constraints}},
year = {2006}
}
@article{Chow2015,
abstract = {In this paper we address the problem of decision making within a Markov decision process (MDP) framework where risk and modeling errors are taken into account. Our approach is to minimize a risk-sensitive conditional-value-at-risk (CVaR) objective, as opposed to a standard risk-neutral expectation. We refer to such problem as CVaR MDP. Our first contribution is to show that a CVaR objective, besides capturing risk sensitivity, has an alternative interpretation as expected cost under worst-case modeling errors, for a given error budget. This result, which is of independent interest, motivates CVaR MDPs as a unifying framework for risk-sensitive and robust decision making. Our second contribution is to present an approximate value-iteration algorithm for CVaR MDPs and analyze its convergence rate. To our knowledge, this is the first solution algorithm for CVaR MDPs that enjoys error guarantees. Finally, we present results from numerical experiments that corroborate our theoretical findings and show the practicality of our approach.},
archivePrefix = {arXiv},
arxivId = {1506.02188},
author = {Chow, Yinlam and Tamar, Aviv and Mannor, Shie and Pavone, Marco},
eprint = {1506.02188},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/3Risk-sensitiveRobustDecisionMaking{\_}cvar.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1522--1530},
title = {{Risk-sensitive and robust decision-making: A CVaR optimization approach}},
volume = {2015-Janua},
year = {2015}
}
@article{Carpin2016,
abstract = {In this paper we present an algorithm to compute risk averse policies in Markov Decision Processes (MDP) when the total cost criterion is used together with the average value at risk (AVaR) metric. Risk averse policies are needed when large deviations from the expected behavior may have detrimental effects, and conventional MDP algorithms usually ignore this aspect. We provide conditions for the structure of the underlying MDP ensuring that approximations for the exact problem can be derived and solved efficiently. Our findings are novel inasmuch as average value at risk has not previously been considered in association with the total cost criterion. Our method is demonstrated in a rapid deployment scenario, whereby a robot is tasked with the objective of reaching a target location within a temporal deadline where increased speed is associated with increased probability of failure. We demonstrate that the proposed algorithm not only produces a risk averse policy reducing the probability of exceeding the expected temporal deadline, but also provides the statistical distribution of costs, thus offering a valuable analysis tool.},
archivePrefix = {arXiv},
arxivId = {1602.05130},
author = {Carpin, Stefano and Chow, Yin Lam and Pavone, Marco},
doi = {10.1109/ICRA.2016.7487152},
eprint = {1602.05130},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/RISK-AVERSE/Risk aversion in finite markov decision processes using total cost criteria and average value at risK.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {335--342},
title = {{Risk aversion in finite Markov Decision Processes using total cost criteria and average value at risk}},
volume = {2016-June},
year = {2016}
}
@inproceedings{Dabney2018b,
abstract = {In this work, we build on recent advances in distributional reinforcement learning to give a generally applicable, flexible, and state-of-the-art distributional variant of DQN. Wc achieve this by using quanlile regression to approximate the full quantile function for the state-action return distri-bution. By reparameterizing a distribution over the sample space, this yields an implicitly defined return distribution and gives rise to a large class of risk-sensitive policies. Wc demonstrate improved performance on the 57 Atari 2600 games in the ALE, and use our algorithm's implicitly defined distributions to study the effects of risk-sensitive policies in Atari games.},
archivePrefix = {arXiv},
arxivId = {1806.06923},
author = {Dabney, Will and Ostrovski, Georg and Silver, David and Munos, Remi},
booktitle = {35th International Conference on Machine Learning, ICML 2018},
eprint = {1806.06923},
isbn = {9781510867963},
title = {{Implicit quantile networks for distributional reinforcement learning}},
year = {2018}
}
@inproceedings{Fujimoto2019,
abstract = {Many practical applications of reinforcement learning constrain agents to learn from a fixed batch of data which has already been gathered, without offering further possibility for data collection. In this paper, we demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of learning without data correlated to the distribution under the current policy, making them ineffective for this fixed batch setting. We introduce a novel class of off-policy algorithms, batch-constrained reinforcement learning, which restricts the action space in order to force the agent towards behaving close to on-policy with respect to a subset of the given data. We present the first continuous control deep reinforcement learning algorithm which can learn effectively from arbitrary, fixed batch data, and empirically demonstrate the quality of its behavior in several tasks.},
archivePrefix = {arXiv},
arxivId = {1812.02900},
author = {Fujimoto, Scott and Meger, David and Precup, Doina},
booktitle = {36th International Conference on Machine Learning, ICML 2019},
eprint = {1812.02900},
isbn = {9781510886988},
title = {{Off-policy deep reinforcement learning without exploration}},
year = {2019}
}
@article{Sutton1998,
abstract = {CiteSeerX - Scientific documents that cite the following paper: Reinforcement learning: An introduction, chapter 11},
author = {Sutton, R.S. and Barto, A.G.},
doi = {10.1109/tnn.1998.712192},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
title = {{Reinforcement Learning: An Introduction}},
year = {1998}
}
@inproceedings{Degris2012,
abstract = {This paper presents the first actor-critic algorithm for off-policy reinforcement learning. Our algorithm is online and incremental, and its per-time-step complexity scales linearly with the number of learned weights. Previous work on actor-critic algorithms is limited to the on-policy setting and does not take advantage of the recent advances in off-policy gradient temporal-difference learning. Off-policy techniques, such as Greedy-GQ, enable a target policy to be learned while following and obtaining data from another (behavior) policy. For many problems, how-ever, actor-critic methods are more practical than action value methods (like Greedy-GQ) because they explicitly represent the policy; consequently, the policy can be stochastic and utilize a large action space. In this paper, we illustrate how to practically combine the generality and learning potential of off-policy learning with the flexibility in action selection given by actor-critic methods. We derive an incremental, linear time and space complexity algorithm that includes eligibility traces, prove convergence under assumptions similar to previous off-policy algorithms, and empirically show better or comparable performance to existing algorithms on standard reinforcement-learning benchmark problems. Copyright 2012 by the author(s)/owner(s).},
author = {Degris, Thomas and White, Martha and Sutton, Richard S.},
booktitle = {Proceedings of the 29th International Conference on Machine Learning, ICML 2012},
isbn = {9781450312851},
title = {{Off-policy actor-critic}},
year = {2012}
}
@article{Rockafellar2002,
abstract = {Fundamental properties of conditional value-at-risk (CVaR), as a measure of risk with significant advantages over value-at-risk (VaR), are derived for loss distributions in finance that can involve discreetness. Such distributions are of particular importance in applications because of the prevalence of models based on scenarios and finite sampling. CVaR is able to quantify dangers beyond VaR and moreover it is coherent. It provides optimization short-cuts which, through linear programming techniques, make practical many large-scale calculations that could otherwise be out of reach. The numerical efficiency and stability of such calculations, shown in several case studies, are illustrated further with an example of index tracking. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
author = {Rockafellar, R. Tyrrell and Uryasev, Stanislav},
doi = {10.1016/S0378-4266(02)00271-6},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/Rockafellar{\_}cvar{\_}for{\_}general{\_}loss{\_}distrib.pdf:pdf},
issn = {03784266},
journal = {Journal of Banking and Finance},
keywords = {Coherent risk measures,Conditional value-at-risk,Hedging,Index tracking,Mean shortfall,Portfolio optimization,Risk management,Risk sampling,Scenarios,Value-at-risk},
number = {7},
pages = {1443--1471},
title = {{Conditional value-at-risk for general loss distributions}},
volume = {26},
year = {2002}
}
@article{Coraluppi1999,
abstract = {This paper analyzes a connection between risk-sensitive and minimax criteria for discrete-time, finite-state Markov decision processes (MDPs). We synthesize optimal policies with respect to both criteria, both for the finite horizon and the discounted infinite horizon problem. A generalized decision-making framework is introduced, which includes as special cases a number of approaches that have been considered in the literature. The framework allows for discounted risk-sensitive and minimax formulations leading to stationary optimal policies on the infinite horizon. We illustrate our results with a simple machine replacement problem. {\textcopyright} 1999 Elsevier Science Ltd. All rights reserved.},
author = {Coraluppi, Stefano P. and Marcus, Steven I.},
doi = {10.1016/S0005-1098(98)00153-8},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/coraluppimarcus.pdf:pdf},
issn = {00051098},
journal = {Automatica},
keywords = {Markov decision processes,Minimax control,Risk-sensitive control,Stochastic Control},
number = {2},
pages = {301--309},
title = {{Risk-sensitive and minimax control of discrete-time, finite-state Markov decision processes}},
volume = {35},
year = {1999}
}
@article{Curi2019,
abstract = {We consider the problem of training machine learning models in a risk-averse manner. In particular, we propose an adaptive sampling algorithm for stochastically optimizing the Conditional Value-at-Risk (CVaR) of a loss distribution. We use a distributionally robust formulation of the CVaR to phrase the problem as a zero-sum game between two players. Our approach solves the game using an efficient no-regret algorithm for each player. Critically, we can apply these algorithms to large-scale settings because the implementation relies on sampling from Determinantal Point Processes. Finally, we empirically demonstrate its effectiveness on large-scale convex and non-convex learning tasks.},
archivePrefix = {arXiv},
arxivId = {1910.12511},
author = {Curi, Sebastian and Levy, Kfir. Y. and Jegelka, Stefanie and Krause, Andreas},
eprint = {1910.12511},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/AdaptiveSampling.pdf:pdf},
title = {{Adaptive Sampling for Stochastic Risk-Averse Learning}},
url = {http://arxiv.org/abs/1910.12511},
year = {2019}
}
@article{Coraluppi1997,
author = {Coraluppi, Stefano P. and Marcus, Steven I.},
journal = {IEEE Transactions on Automatic Control},
number = {3},
pages = {528--532},
title = {{Mixed risk-neutral/minimax control of discrete-time, finite-state Markov decision processes}},
volume = {45},
year = {2000}
}
@article{Kumar2019,
abstract = {Off-policy reinforcement learning aims to leverage experience collected from prior policies for sample-efficient learning. However, in practice, commonly used off-policy approximate dynamic programming methods based on Q-learning and actor-critic methods are highly sensitive to the data distribution, and can make only limited progress without collecting additional on-policy data. As a step towards more robust off-policy algorithms, we study the setting where the off-policy experience is fixed and there is no further interaction with the environment. We identify bootstrapping error as a key source of instability in current methods. Bootstrapping error is due to bootstrapping from actions that lie outside of the training data distribution, and it accumulates via the Bellman backup operator. We theoretically analyze bootstrapping error, and demonstrate how carefully constraining action selection in the backup can mitigate it. Based on our analysis, we propose a practical algorithm, bootstrapping error accumulation reduction (BEAR). We demonstrate that BEAR is able to learn robustly from different off-policy distributions, including random and suboptimal demonstrations, on a range of continuous control tasks.},
archivePrefix = {arXiv},
arxivId = {1906.00949},
author = {Kumar, Aviral and Fu, Justin and Tucker, George and Levine, Sergey},
eprint = {1906.00949},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/BEAR.pdf:pdf},
number = {NeurIPS},
title = {{Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction}},
url = {http://arxiv.org/abs/1906.00949},
year = {2019}
}
@article{Tamar2017,
abstract = {We provide sampling-based algorithms for optimization under a coherent-risk objective. The class of coherent-risk measures is widely accepted in finance and operations research, among other fields, and encompasses popular risk-measures such as conditional value at risk and mean-semi-deviation. Our approach is suitable for problems in which tuneable parameters control the distribution of the cost, such as in reinforcement learning or approximate dynamic programming with a parameterized policy. Such problems cannot be solved using previous approaches. We consider both static risk measures and time-consistent dynamic risk measures. For static risk measures, our approach is in the spirit of policy gradient methods, while for the dynamic risk measures, we use actor-critic type algorithms.},
author = {Tamar, Aviv and Chow, Yinlam and Ghavamzadeh, Mohammad and Mannor, Shie},
doi = {10.1109/TAC.2016.2644871},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/Sequential Decision Making With Coherent Risk.pdf:pdf},
issn = {00189286},
journal = {IEEE Transactions on Automatic Control},
keywords = {Coherent risk,Markov decision processes,dynamic programming,policy gradient},
number = {7},
pages = {3323--3338},
publisher = {IEEE},
title = {{Sequential Decision Making with Coherent Risk}},
volume = {62},
year = {2017}
}
@article{Marcus1997,
author = {Marcus, Steven I},
doi = {10.1007/978-1-4612-4120-1},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/Risk{\_}Sensitive{\_}Markov{\_}Decision{\_}Processes.pdf:pdf},
isbn = {9781461241201},
journal = {Systems and Control in the Twenty-First Century},
number = {January 1997},
title = {{Systems and Control in the Twenty-First Century}},
year = {1997}
}
@inproceedings{Lillicrap2016,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies “end-to-end”: directly from raw pixel inputs.},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
booktitle = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
eprint = {1509.02971},
title = {{Continuous control with deep reinforcement learning}},
year = {2016}
}
@article{Altman1993,
abstract = {We present in this paper several asymptotic properties of constrained Markov Decision Processes (MDPs) with a countable state space. We treat both the discounted and the expected average cost, with unbounded cost. We are interested in (1) the convergence of finite horizon MDPs to the infinite horizon MDP, (2) convergence of MDPs with a truncated state space to the problem with infinite state space, (3) convergence of MDPs as the discount factor goes to a limit. In all these cases we establish the convergence of optimal values and policies. Moreover, based on the optimal policy for the limiting problem, we construct policies which are almost optimal for the other (approximating) problems. Based on the convergence of MDPs with a truncated state space to the problem with infinite state space, we show that an optimal stationary policy exists such that the number of randomisations it uses is less or equal to the number of constraints plus one. We finally apply the results to a dynamic scheduling problem. {\textcopyright} 1993 Physica-Verlag.},
author = {Altman, Eitan},
doi = {10.1007/BF01414154},
issn = {03409422},
journal = {ZOR - Methods and Models of Operations Research},
keywords = {Constrained Markov Decision Processes,asymptotic properties,countable state space,dynamic scheduling,finite approximations,finite horizon,infinite horizon},
title = {{Asymptotic properties of constrained Markov Decision Processes}},
year = {1993}
}
@article{Tamar2015,
abstract = {Several authors have recently developed risk-sensitive policy gradient methods that augment the standard expected cost minimization problem with a measure of variability in cost. These studies have focused on specific risk-measures, such as the variance or conditional value at risk (CVaR). In this work, we extend the policy gradient method to the whole class of coherent risk measures, which is widely accepted in finance and operations research, among other fields. We consider both static and time-consistent dynamic risk measures. For static risk measures, our approach is in the spirit of policy gradient algorithms and combines a standard sampling approach with convex programming. For dynamic risk measures, our approach is actor-critic style and involves explicit approximation of value function. Most importantly, our contribution presents a unified approach to risk-sensitive reinforcement learning that generalizes and extends previous results.},
archivePrefix = {arXiv},
arxivId = {1502.03919},
author = {Tamar, Aviv and Chow, Yinlam and Ghavamzadeh, Mohammad and Mannor, Shie},
eprint = {1502.03919},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/Policy-gradient-for-coherent-risk-measures.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1468--1476},
title = {{Policy gradient for coherent risk measures}},
volume = {2015-Janua},
year = {2015}
}
@misc{Garcia2015,
abstract = {Safe Reinforcement Learning can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes. We categorize and analyze two approaches of Safe Reinforcement Learning. The first is based on the modification of the optimality criterion, the classic discounted finite/infinite horizon, with a safety factor. The second is based on the modification of the exploration process through the incorporation of external knowledge or the guidance of a risk metric. We use the proposed classification to survey the existing literature, as well as suggesting future directions for Safe Reinforcement Learning.},
author = {Garc{\'{i}}a, Javier and Fern{\'{a}}ndez, Fernando},
booktitle = {Journal of Machine Learning Research},
issn = {15337928},
keywords = {Reinforcement learning,Risk sensitivity,Safe exploration,Teacher advice},
title = {{A comprehensive survey on safe reinforcement learning}},
year = {2015}
}
@article{Howard1972,
abstract = {This paper considers the maximization of certain equivalent reward generated by a Markov decision process with constant risk sensitivity. First, value iteration is used to optimize possibly time-varying processes of finite duration. Then a policy iteration procedure is developed to find the stationary policy with highest certain equivalent gain for the infinite duration case. A simple example demonstrates both procedures.},
author = {Howard, Ronald A and Matheson, James E},
issn = {00251909, 15265501},
journal = {Management Science},
number = {7},
pages = {356--369},
publisher = {INFORMS},
title = {{Risk-Sensitive Markov Decision Processes}},
url = {http://www.jstor.org/stable/2629352},
volume = {18},
year = {1972}
}
@article{Watkins1992,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem forQ-learning based on that outlined in Watkins (1989). We show thatQ-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where manyQ values can be changed each iteration, rather than just one.},
author = {Watkins, Christopher J. C. H. and Dayan, Peter},
doi = {10.1007/bf00992698},
issn = {0885-6125},
journal = {Machine Learning},
title = {{Q-learning}},
year = {1992}
}
@article{Artzner1999,
abstract = {In this paper we study both market risks and nonmarket risks, without complete markets assumption, and discuss methods of measurement of these risks. We present and justify a set of four desirable properties for measures of risk, and call the measures satisfying these properties "coherent." We examine the measures of risk provided and the related actions required by SPAN, by the SEC/NASD rules, and by quantile-based methods. We demonstrate the universality of scenario-based methods for providing coherent measures. We offer suggestions concerning the SEC method. We also suggest a method to repair the failure of subadditivity of quantile-based methods.},
author = {Artzner, Philippe and Delbaen, Freddy and Eber, Jean Marc and Heath, David},
doi = {10.1111/1467-9965.00068},
issn = {09601627},
journal = {Mathematical Finance},
keywords = {Aggregation of risks,Butterfly,Capital requirement,Coherent risk measure,Concentration of risks,Currency risk,Decentralization,Extremal events risk,Insurance risk,Margin requirement,Market risk,Mean excess function,Measure of risk,Model risk},
title = {{Coherent measures of risk}},
year = {1999}
}
@article{Rockafellar2000,
abstract = {A new approach to optimizing or hedging a portfolio of financial instruments to reduce risk is presented and tested on applications. It focuses on minimizing conditional value-at-risk (CVaR) rather than minimizing value-at-risk (VaR), but portfolios with low CVaR necessarily have low VaR as well. CVaR, also called mean excess loss, mean shortfall, or tail VaR, is in any case considered to be a more consistent measure of risk than VaR. Central to the new approach is a technique for portfolio optimization which calculates VaR and optimizes CVaR simultaneously. This technique is suitable for use by investment companies, brokerage firms, mutual funds, and any business that evaluates risk. It can be combined with analytical or scenario-based methods to optimize portfolios with large numbers of instruments, in which case the calculations often come down to linear programming or nonsmooth programming. The methodology can also be applied to the optimization of percentiles in contexts outside of finance.},
author = {Rockafellar, R. Tyrrell and Uryasev, Stanislav},
doi = {10.21314/jor.2000.038},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/important{\_}cvar{\_}read/2CVAR.pdf:pdf},
issn = {14651211},
journal = {The Journal of Risk},
number = {3},
pages = {21--41},
title = {{Optimization of conditional value-at-risk}},
volume = {2},
year = {2000}
}
@inproceedings{Petrik2012,
abstract = {Stochastic domains often involve risk-averse decision makers. While recent work has focused on how to model risk in Markov decision processes using risk measures, it has not addressed the problem of solving large risk-averse formulations. In this paper, we propose and analyze a new method for solving large risk-averse MDPs with hybrid continuous-discrete state spaces and continuous action spaces. The proposed method iteratively improves a bound on the value function using a linearity structure of the MDP. We demonstrate the utility and properties of the method on a portfolio optimization problem.},
author = {Petrik, Marek and Subramanian, Dharmashankar},
booktitle = {Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012},
isbn = {9780974903989},
title = {{An approximate solution method for large risk-averse markov decision processes}},
year = {2012}
}
@article{Morimura2010,
abstract = {Most conventional Reinforcement Learning (RL) algorithms aim to optimize decision-making rules in terms of the expected re-turns. However, especially for risk management purposes, other risk-sensitive criteria such as the value-at-risk or the expected shortfall are sometimes preferred in real applications. Here, we describe a parametric method for estimating density of the returns, which allows us to handle various criteria in a unified manner. We first extend the Bellman equation for the conditional expected return to cover a conditional probability density of the returns. Then we derive an extension of the TD-learning algorithm for estimating the return densities in an unknown environment. As test instances, several parametric density estimation algorithms are presented for the Gaussian, Laplace, and skewed Laplace distributions. We show that these algorithms lead to risk-sensitive as well as robust RL paradigms through numerical experiments.},
archivePrefix = {arXiv},
arxivId = {1203.3497},
author = {Morimura, Tetsuro and Sugiyama, Masashi and Kashima, Hisashi and Hachiya, Hirotaka and Tanaka, Toshiyuki},
eprint = {1203.3497},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/distributional{\_}RL/parametric{\_}return{\_}density{\_}estimation{\_}for{\_}RL.pdf:pdf},
isbn = {9780974903965},
journal = {Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence, UAI 2010},
pages = {368--375},
title = {{Parametric return density estimation for Reinforcement Learning}},
year = {2010}
}
@article{Silver2014b,
abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. Deterministic policy gradient algorithms outperformed their stochastic counterparts in several benchmark problems, particularly in high-dimensional action spaces.},
author = {Silver, David and Lever, Guy and Technologies, Deepmind and Lever, G U Y and Ac, U C L},
isbn = {9781634393973},
issn = {1938-7228},
journal = {Proceedings of the 31st International Conference on Machine Learning},
title = {{Deterministic Policy Gradient (DPG)}},
year = {2014}
}
@incollection{Heger1994,
abstract = {Most Reinforcement Learning (RL) work supposes policies for sequential decision$\backslash$ntasks to be optimal that minimize the expected total discounted cost (e. g. Q-Learning$\backslash$n[Wat 89], AHC [Bar Sut And 83]). On the other hand, it is well known$\backslash$nthat it is not always reliable and can be treacherous to use the expected value as a$\backslash$ndecision criterion [Tha 87]. A lot of alternative decision criteria have been$\backslash$nsuggested in decision theory to get a more sophisticated considaration of risk but$\backslash$nmost RL researchers have not concerned themselves with this subject until now.$\backslash$nThe purpose of this paper is to draw the reader's attention to the problems of the$\backslash$nexpected value criterion in Markov Decision Processes and to give Dynamic$\backslash$nProgramming algorithms for an alternative criterion, namely the Minimax criterion.$\backslash$nA counterpart to Watkins' Q-Learning related to the Minimax criterion is$\backslash$npresented. The new algorithm, called Q − Learning , finds policies that minimize the$\backslash$nworst-case total discounted costs. Most mathematical details aren't presented here$\backslash$nbut can be found in [Heg 94].},
author = {Heger, Matthias},
booktitle = {Machine Learning Proceedings 1994},
doi = {10.1016/b978-1-55860-335-6.50021-0},
title = {{Consideration of Risk in Reinforcement Learning}},
year = {1994}
}
@inproceedings{Sohn2015,
abstract = {Supervised deep learning has been successfully applied to many recognition problems. Although it can approximate a complex many-to-one function well when a large amount of training data is provided, it is still challenging to model complex structured output representations that effectively perform probabilistic inference and make diverse predictions. In this work, we develop a deep conditional generative model for structured output prediction using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient variational Bayes, and allows for fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build robust structured prediction algorithms, such as input noise-injection and multi-scale prediction objective at training. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic inference. Furthermore, the proposed training methods are complimentary, which leads to strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
author = {Sohn, Kihyuk and Yan, Xinchen and Lee, Honglak},
booktitle = {Advances in Neural Information Processing Systems},
issn = {10495258},
title = {{Learning structured output representation using deep conditional generative models}},
year = {2015}
}
@inbook{Bertsekas1995,
author = {Bertsekas, Dimitri},
title = {{Dynamic Programming and Optimal Control}},
volume = {1},
year = {1995}
}
@inproceedings{Bellemare2017,
abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
archivePrefix = {arXiv},
arxivId = {1707.06887},
author = {Bellemare, Marc G. and Dabney, Will and Munos, R{\'{e}}mi},
booktitle = {34th International Conference on Machine Learning, ICML 2017},
eprint = {1707.06887},
isbn = {9781510855144},
title = {{A distributional perspective on reinforcement learning}},
year = {2017}
}
