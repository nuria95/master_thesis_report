Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@misc{d4rl,
archivePrefix = {arXiv},
arxivId = {cs.LG/2004.07219},
author = {Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
eprint = {2004.07219},
primaryClass = {cs.LG},
title = {{D4RL: Datasets for Deep Data-Driven Reinforcement Learning}},
year = {2020}
}
@inproceedings{Morimura2010a,
abstract = {Most conventional Reinforcement Learning (RL) algorithms aim to optimize decision-making rules in terms of the expected re-turns. However, especially for risk management purposes, other risk-sensitive criteria such as the value-at-risk or the expected shortfall are sometimes preferred in real applications. Here, we describe a parametric method for estimating density of the returns, which allows us to handle various criteria in a unified manner. We first extend the Bellman equation for the conditional expected return to cover a conditional probability density of the returns. Then we derive an extension of the TD-learning algorithm for estimating the return densities in an unknown environment. As test instances, several parametric density estimation algorithms are presented for the Gaussian, Laplace, and skewed Laplace distributions. We show that these algorithms lead to risk-sensitive as well as robust RL paradigms through numerical experiments.},
archivePrefix = {arXiv},
arxivId = {1203.3497},
author = {Morimura, Tetsuro and Sugiyama, Masashi and Kashima, Hisashi and Hachiya, Hirotaka and Tanaka, Toshiyuki},
booktitle = {Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence, UAI 2010},
eprint = {1203.3497},
isbn = {9780974903965},
title = {{Parametric return density estimation for Reinforcement Learning}},
year = {2010}
}
@article{Rockafellar2002,
abstract = {Fundamental properties of conditional value-at-risk (CVaR), as a measure of risk with significant advantages over value-at-risk (VaR), are derived for loss distributions in finance that can involve discreetness. Such distributions are of particular importance in applications because of the prevalence of models based on scenarios and finite sampling. CVaR is able to quantify dangers beyond VaR and moreover it is coherent. It provides optimization short-cuts which, through linear programming techniques, make practical many large-scale calculations that could otherwise be out of reach. The numerical efficiency and stability of such calculations, shown in several case studies, are illustrated further with an example of index tracking. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
author = {Rockafellar, R. Tyrrell and Uryasev, Stanislav},
doi = {10.1016/S0378-4266(02)00271-6},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/Rockafellar{\_}cvar{\_}for{\_}general{\_}loss{\_}distrib.pdf:pdf},
issn = {03784266},
journal = {Journal of Banking and Finance},
keywords = {Coherent risk measures,Conditional value-at-risk,Hedging,Index tracking,Mean shortfall,Portfolio optimization,Risk management,Risk sampling,Scenarios,Value-at-risk},
number = {7},
pages = {1443--1471},
title = {{Conditional value-at-risk for general loss distributions}},
volume = {26},
year = {2002}
}
@inproceedings{Syed2008,
abstract = {In apprenticeship learning, the goal is to learn a policy in a Markov decision process that is at least as good as a policy demonstrated by an expert. The difficulty arises in that the MDP's true reward function is assumed to be unknown. We show how to frame apprenticeship learning as a linear programming problem, and show that using an off-the-shelf LP solver to solve this problem results in a substantial improvement in running time over existing methods - up to two orders of magnitude faster in our experiments. Additionally, our approach produces stationary policies, while all existing methods for apprenticeship learning output policies that are "mixed", i.e. randomized combinations of stationary policies. The technique used is general enough to convert any mixed policy to a stationary policy. Copyright 2008 by the author(s)/owner(s).},
author = {Syed, Umar and Bowling, Michael and Schapire, Robert E.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
doi = {10.1145/1390156.1390286},
isbn = {9781605582054},
title = {{Apprenticeship learning using linear programming}},
year = {2008}
}
@article{Haarnoja2018,
archivePrefix = {arXiv},
arxivId = {1812.05905},
author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
eprint = {1812.05905},
journal = {CoRR},
title = {{Soft Actor-Critic Algorithms and Applications}},
url = {http://arxiv.org/abs/1812.05905},
volume = {abs/1812.0},
year = {2018}
}
@article{Curi2019,
abstract = {We consider the problem of training machine learning models in a risk-averse manner. In particular, we propose an adaptive sampling algorithm for stochastically optimizing the Conditional Value-at-Risk (CVaR) of a loss distribution. We use a distributionally robust formulation of the CVaR to phrase the problem as a zero-sum game between two players. Our approach solves the game using an efficient no-regret algorithm for each player. Critically, we can apply these algorithms to large-scale settings because the implementation relies on sampling from Determinantal Point Processes. Finally, we empirically demonstrate its effectiveness on large-scale convex and non-convex learning tasks.},
archivePrefix = {arXiv},
arxivId = {1910.12511},
author = {Curi, Sebastian and Levy, Kfir. Y. and Jegelka, Stefanie and Krause, Andreas},
eprint = {1910.12511},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/AdaptiveSampling.pdf:pdf},
title = {{Adaptive Sampling for Stochastic Risk-Averse Learning}},
url = {http://arxiv.org/abs/1910.12511},
year = {2019}
}
@inproceedings{Dabney2018a,
abstract = {In reinforcement learning (RL), an agent interacts with the environment by taking actions and observing the next state and reward. When sampled probabilistically, these state transitions, rewards, and actions can all induce randomness in the observed long-term return. Traditionally, reinforcement learning algorithms average over this randomness to estimate the value function. In this paper, we build on recent work advocating a distributional approach to reinforcement learning in which the distribution over returns is modeled explicitly instead of only estimating the mean. That is, we examine methods of learning the value distribution instead of the value function. We give results that close a number of gaps between the theoretical and algorithmic results given by Bellemare, Dabney, and Munos (2017). First, we extend existing results to the approximate distribution setting. Second, we present a novel distributional reinforcement learning algorithm consistent with our theoretical formulation. Finally, we evaluate this new algorithm on the Atari 2600 games, observing that it significantly outperforms many of the recent improvements on DQN, including the related distributional algorithm C51.},
archivePrefix = {arXiv},
arxivId = {1710.10044},
author = {Dabney, Will and Rowland, Mark and Bellemare, Marc G. and Munos, R{\'{e}}mi},
booktitle = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
eprint = {1710.10044},
isbn = {9781577358008},
title = {{Distributional reinforcement learning with quantile regression}},
year = {2018}
}
@incollection{Heger1994,
abstract = {Most Reinforcement Learning (RL) work supposes policies for sequential decision$\backslash$ntasks to be optimal that minimize the expected total discounted cost (e. g. Q-Learning$\backslash$n[Wat 89], AHC [Bar Sut And 83]). On the other hand, it is well known$\backslash$nthat it is not always reliable and can be treacherous to use the expected value as a$\backslash$ndecision criterion [Tha 87]. A lot of alternative decision criteria have been$\backslash$nsuggested in decision theory to get a more sophisticated considaration of risk but$\backslash$nmost RL researchers have not concerned themselves with this subject until now.$\backslash$nThe purpose of this paper is to draw the reader's attention to the problems of the$\backslash$nexpected value criterion in Markov Decision Processes and to give Dynamic$\backslash$nProgramming algorithms for an alternative criterion, namely the Minimax criterion.$\backslash$nA counterpart to Watkins' Q-Learning related to the Minimax criterion is$\backslash$npresented. The new algorithm, called Q − Learning , finds policies that minimize the$\backslash$nworst-case total discounted costs. Most mathematical details aren't presented here$\backslash$nbut can be found in [Heg 94].},
author = {Heger, Matthias},
booktitle = {Machine Learning Proceedings 1994},
doi = {10.1016/b978-1-55860-335-6.50021-0},
title = {{Consideration of Risk in Reinforcement Learning}},
year = {1994}
}
@inproceedings{Degris2012,
abstract = {This paper presents the first actor-critic algorithm for off-policy reinforcement learning. Our algorithm is online and incremental, and its per-time-step complexity scales linearly with the number of learned weights. Previous work on actor-critic algorithms is limited to the on-policy setting and does not take advantage of the recent advances in off-policy gradient temporal-difference learning. Off-policy techniques, such as Greedy-GQ, enable a target policy to be learned while following and obtaining data from another (behavior) policy. For many problems, how-ever, actor-critic methods are more practical than action value methods (like Greedy-GQ) because they explicitly represent the policy; consequently, the policy can be stochastic and utilize a large action space. In this paper, we illustrate how to practically combine the generality and learning potential of off-policy learning with the flexibility in action selection given by actor-critic methods. We derive an incremental, linear time and space complexity algorithm that includes eligibility traces, prove convergence under assumptions similar to previous off-policy algorithms, and empirically show better or comparable performance to existing algorithms on standard reinforcement-learning benchmark problems. Copyright 2012 by the author(s)/owner(s).},
author = {Degris, Thomas and White, Martha and Sutton, Richard S.},
booktitle = {Proceedings of the 29th International Conference on Machine Learning, ICML 2012},
isbn = {9781450312851},
title = {{Off-policy actor-critic}},
year = {2012}
}
@article{Tamar2015,
abstract = {Several authors have recently developed risk-sensitive policy gradient methods that augment the standard expected cost minimization problem with a measure of variability in cost. These studies have focused on specific risk-measures, such as the variance or conditional value at risk (CVaR). In this work, we extend the policy gradient method to the whole class of coherent risk measures, which is widely accepted in finance and operations research, among other fields. We consider both static and time-consistent dynamic risk measures. For static risk measures, our approach is in the spirit of policy gradient algorithms and combines a standard sampling approach with convex programming. For dynamic risk measures, our approach is actor-critic style and involves explicit approximation of value function. Most importantly, our contribution presents a unified approach to risk-sensitive reinforcement learning that generalizes and extends previous results.},
archivePrefix = {arXiv},
arxivId = {1502.03919},
author = {Tamar, Aviv and Chow, Yinlam and Ghavamzadeh, Mohammad and Mannor, Shie},
eprint = {1502.03919},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/Policy-gradient-for-coherent-risk-measures.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1468--1476},
title = {{Policy gradient for coherent risk measures}},
volume = {2015-Janua},
year = {2015}
}
@article{Tamar2015a,
abstract = {Conditional Value at Risk (CVaR) is a prominent risk measure that is being used extensively in various domains. We develop a new formula for the gradient of the CVaR in the form of a conditional expectation. Based on this formula, we propose a novel sampling-based estimator for the gradient of the CVaR, in the spirit of the likelihood-ratio method. We analyze the bias of the estimator, and prove the convergence of a corresponding stochastic gradient descent algorithm to a local CVaR optimum. Our method allows to consider CVaR optimization in new domains. As an example, we consider a reinforcement learning application, and learn a risksensitive controller for the game of Tetris.},
archivePrefix = {arXiv},
arxivId = {1404.3862},
author = {Tamar, Aviv and Glassner, Yonatan and Mannor, Shie},
eprint = {1404.3862},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/new{\_}from{\_}friday{\_}9429-45152-1-PB.pdf:pdf},
isbn = {9781577357025},
journal = {Proceedings of the National Conference on Artificial Intelligence},
keywords = {Novel Machine Learning Algorithms Track},
pages = {2993--2999},
title = {{Optimizing the CVaR via sampling}},
volume = {4},
year = {2015}
}
@inproceedings{Morimura2010b,
abstract = {Standard Reinforcement Learning (RL) aims to optimize decision-making rules in terms of the expected return. However, especially for risk-management purposes, other criteria such as the expected shortfall are some-times preferred. Here, we describe a method of approximating the distribution of returns, which allows us to derive various kinds of information about the returns. We first show that the Bellman equation, which is a recursive formula for the expected return, can be extended to the cumulative return distribution. Then we derive a nonparametric return distribution estimator with particle smooth ing based on this extended Bellman equation. A key aspect of the proposed algorithm is to represent the recursion relation in the extended Bellman equation by a simple replacement procedure of particles associated with a state by using those of the successor state. We show that our algorithm leads to a risk-sensitive R.L paradigm. The usefulness of the proposed approach is demonstrated through numerical experiments. Copyright 2010 by the author(s)/owner(s).},
author = {{Morimura, Tetsuro and Sugiyama, Masashi and Kashima, Hisashi and Hachiya, Hirotaka and Tanaka}, Toshiyuki},
booktitle = {ICML 2010 - Proceedings, 27th International Conference on Machine Learning},
isbn = {9781605589077},
title = {{Nonparametric return distribution approximation for reinforcement learning}},
year = {2010}
}
@article{Watkins1992,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem forQ-learning based on that outlined in Watkins (1989). We show thatQ-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where manyQ values can be changed each iteration, rather than just one.},
author = {Watkins, Christopher J. C. H. and Dayan, Peter},
doi = {10.1007/bf00992698},
issn = {0885-6125},
journal = {Machine Learning},
title = {{Q-learning}},
year = {1992}
}
@incollection{Serraino2013,
author = {Serraino, Gaia and Uryasev, Stanislav},
booktitle = {Encyclopedia of Operations Research and Management Science},
doi = {10.1007/978-1-4419-1153-7_1232},
title = {{Conditional Value-at-Risk (CVaR)}},
year = {2013}
}
@inproceedings{Bellemare2017,
abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
archivePrefix = {arXiv},
arxivId = {1707.06887},
author = {Bellemare, Marc G. and Dabney, Will and Munos, R{\'{e}}mi},
booktitle = {34th International Conference on Machine Learning, ICML 2017},
eprint = {1707.06887},
isbn = {9781510855144},
title = {{A distributional perspective on reinforcement learning}},
year = {2017}
}
@inproceedings{Fujimoto2019,
abstract = {Many practical applications of reinforcement learning constrain agents to learn from a fixed batch of data which has already been gathered, without offering further possibility for data collection. In this paper, we demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of learning without data correlated to the distribution under the current policy, making them ineffective for this fixed batch setting. We introduce a novel class of off-policy algorithms, batch-constrained reinforcement learning, which restricts the action space in order to force the agent towards behaving close to on-policy with respect to a subset of the given data. We present the first continuous control deep reinforcement learning algorithm which can learn effectively from arbitrary, fixed batch data, and empirically demonstrate the quality of its behavior in several tasks.},
archivePrefix = {arXiv},
arxivId = {1812.02900},
author = {Fujimoto, Scott and Meger, David and Precup, Doina},
booktitle = {36th International Conference on Machine Learning, ICML 2019},
eprint = {1812.02900},
isbn = {9781510886988},
title = {{Off-policy deep reinforcement learning without exploration}},
year = {2019}
}
@article{Kumar2019,
abstract = {Off-policy reinforcement learning aims to leverage experience collected from prior policies for sample-efficient learning. However, in practice, commonly used off-policy approximate dynamic programming methods based on Q-learning and actor-critic methods are highly sensitive to the data distribution, and can make only limited progress without collecting additional on-policy data. As a step towards more robust off-policy algorithms, we study the setting where the off-policy experience is fixed and there is no further interaction with the environment. We identify bootstrapping error as a key source of instability in current methods. Bootstrapping error is due to bootstrapping from actions that lie outside of the training data distribution, and it accumulates via the Bellman backup operator. We theoretically analyze bootstrapping error, and demonstrate how carefully constraining action selection in the backup can mitigate it. Based on our analysis, we propose a practical algorithm, bootstrapping error accumulation reduction (BEAR). We demonstrate that BEAR is able to learn robustly from different off-policy distributions, including random and suboptimal demonstrations, on a range of continuous control tasks.},
archivePrefix = {arXiv},
arxivId = {1906.00949},
author = {Kumar, Aviral and Fu, Justin and Tucker, George and Levine, Sergey},
eprint = {1906.00949},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/BEAR.pdf:pdf},
number = {NeurIPS},
title = {{Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction}},
url = {http://arxiv.org/abs/1906.00949},
year = {2019}
}
@article{Howard1972,
abstract = {This paper considers the maximization of certain equivalent reward generated by a Markov decision process with constant risk sensitivity. First, value iteration is used to optimize possibly time-varying processes of finite duration. Then a policy iteration procedure is developed to find the stationary policy with highest certain equivalent gain for the infinite duration case. A simple example demonstrates both procedures.},
author = {Howard, Ronald A and Matheson, James E},
issn = {00251909, 15265501},
journal = {Management Science},
number = {7},
pages = {356--369},
publisher = {INFORMS},
title = {{Risk-Sensitive Markov Decision Processes}},
url = {http://www.jstor.org/stable/2629352},
volume = {18},
year = {1972}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
issn = {14764687},
journal = {Nature},
pmid = {25719670},
title = {{Human-level control through deep reinforcement learning}},
year = {2015}
}
@article{Marcus1997,
author = {Marcus, Steven I},
doi = {10.1007/978-1-4612-4120-1},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/Risk{\_}Sensitive{\_}Markov{\_}Decision{\_}Processes.pdf:pdf},
isbn = {9781461241201},
journal = {Systems and Control in the Twenty-First Century},
number = {January 1997},
title = {{Systems and Control in the Twenty-First Century}},
year = {1997}
}
@article{Chung1987,
abstract = {The present value of the rewards associated with a discrete-time Markov process has a probability distribution which depends on the initial state. The first part of the paper applies fixed point theory to a system of equations for the distribution functions of the present value. The second part of the paper expands the model to a Markov decision process (MDP) and considers the maximization of the expected utility of the present value when the utility function is exponential.},
author = {Chung, Kun Jen and Sobel, Matthew J.},
doi = {10.1137/0325004},
issn = {03630129},
journal = {SIAM Journal on Control and Optimization},
title = {{Discounted MDPs: Distribution functions and exponential utility maximization}},
year = {1987}
}
@article{Bertsekas1995,
abstract = {These proceedings are based on lectures delivered at the Symposium on Nonlinear Programming held March 23 and 24, 1975, as part of the American Mathematical Society's annual New York meeting. The purpose of the symposium was to help bring to the attention of a larger mathematical audience some of the history, theory, applications and vigorous research activity of the Nonlinear Programming field. The program was largely tutorial, although excellent talks on new results were purposely included and the speakers were encouraged to identify promising regions for further exploration. The eight papers presented deal with such aspects as optimality conditions, algorithms and their convergence, approximate solutions, and optimization.},
author = {Bertsekas, Dimitri},
doi = {10.1016/b978-1-4831-9795-1.50011-5},
journal = {SIAM AMS Proc},
title = {{Nonlinear Programming.}},
volume = {9},
year = {1976}
}
@inproceedings{Moldovan2012,
abstract = {In environments with uncertain dynamics exploration is necessary to learn how to perform well. Existing reinforcement learning algorithms provide strong exploration guarantees, but they tend to rely on an ergodicity assumption. The essence of ergodicity is that any state is eventually reachable from any other state by following a suitable policy. This assumption allows for exploration algorithms that operate by simply favoring states that have rarely been visited before. For most physical systems this assumption is impractical as the systems would break before any reasonable exploration has taken place, i.e., most physical systems don't satisfy the ergodicity assumption. In this paper we address the need for safe exploration methods in Markov decision processes. We first propose a general formulation of safety through ergodicity. We show that imposing safety by restricting attention to the resulting set of guaranteed safe policies is NP-hard. We then present an efficient algorithm for guaranteed safe, but potentially suboptimal, exploration. At the core is an optimization formulation in which the constraints restrict attention to a subset of the guaranteed safe policies and the objective favors exploration policies. Our framework is compatible with the majority of previously proposed exploration methods, which rely on an exploration bonus. Our experiments, which include a Martian terrain exploration problem, show that our method is able to explore better than classical exploration methods. Copyright 2012 by the author(s)/owner(s).},
archivePrefix = {arXiv},
arxivId = {1205.4810},
author = {Moldovan, Teodor Mihai and Abbeel, Pieter},
booktitle = {Proceedings of the 29th International Conference on Machine Learning, ICML 2012},
eprint = {1205.4810},
isbn = {9781450312851},
title = {{Safe exploration in Markov decision processes}},
year = {2012}
}
@article{Silver2014b,
abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. Deterministic policy gradient algorithms outperformed their stochastic counterparts in several benchmark problems, particularly in high-dimensional action spaces.},
author = {Silver, David and Lever, Guy and Technologies, Deepmind and Lever, G U Y and Ac, U C L},
isbn = {9781634393973},
issn = {1938-7228},
journal = {Proceedings of the 31st International Conference on Machine Learning},
title = {{Deterministic Policy Gradient (DPG)}},
year = {2014}
}
@article{Sutton1998,
abstract = {CiteSeerX - Scientific documents that cite the following paper: Reinforcement learning: An introduction, chapter 11},
author = {Sutton, R.S. and Barto, A.G.},
doi = {10.1109/tnn.1998.712192},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
title = {{Reinforcement Learning: An Introduction}},
year = {1998}
}
@article{Carpin2016,
abstract = {In this paper we present an algorithm to compute risk averse policies in Markov Decision Processes (MDP) when the total cost criterion is used together with the average value at risk (AVaR) metric. Risk averse policies are needed when large deviations from the expected behavior may have detrimental effects, and conventional MDP algorithms usually ignore this aspect. We provide conditions for the structure of the underlying MDP ensuring that approximations for the exact problem can be derived and solved efficiently. Our findings are novel inasmuch as average value at risk has not previously been considered in association with the total cost criterion. Our method is demonstrated in a rapid deployment scenario, whereby a robot is tasked with the objective of reaching a target location within a temporal deadline where increased speed is associated with increased probability of failure. We demonstrate that the proposed algorithm not only produces a risk averse policy reducing the probability of exceeding the expected temporal deadline, but also provides the statistical distribution of costs, thus offering a valuable analysis tool.},
archivePrefix = {arXiv},
arxivId = {1602.05130},
author = {Carpin, Stefano and Chow, Yin Lam and Pavone, Marco},
doi = {10.1109/ICRA.2016.7487152},
eprint = {1602.05130},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/RISK-AVERSE/Risk aversion in finite markov decision processes using total cost criteria and average value at risK.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {335--342},
title = {{Risk aversion in finite Markov Decision Processes using total cost criteria and average value at risk}},
volume = {2016-June},
year = {2016}
}
@inproceedings{Kadota2006,
abstract = {We consider utility-constrained Markov decision processes. The expected utility of the total discounted reward is maximized subject to multiple expected utility constraints. By introducing a corresponding Lagrange function, a saddle-point theorem of the utility constrained optimization is derived. The existence of a constrained optimal policy is characterized by optimal action sets specified with a parametric utility. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
author = {Kadota, Yoshinobu and Kurano, Masami and Yasuda, Masami},
booktitle = {Computers and Mathematics with Applications},
doi = {10.1016/j.camwa.2005.11.013},
issn = {08981221},
keywords = {Constrained optimal policy,Discount criterion,Lagrange technique,Markov decision processes,Saddle-point,Utility constraints},
title = {{Discounted Markov decision processes with utility constraints}},
year = {2006}
}
@article{Chow2015,
abstract = {In this paper we address the problem of decision making within a Markov decision process (MDP) framework where risk and modeling errors are taken into account. Our approach is to minimize a risk-sensitive conditional-value-at-risk (CVaR) objective, as opposed to a standard risk-neutral expectation. We refer to such problem as CVaR MDP. Our first contribution is to show that a CVaR objective, besides capturing risk sensitivity, has an alternative interpretation as expected cost under worst-case modeling errors, for a given error budget. This result, which is of independent interest, motivates CVaR MDPs as a unifying framework for risk-sensitive and robust decision making. Our second contribution is to present an approximate value-iteration algorithm for CVaR MDPs and analyze its convergence rate. To our knowledge, this is the first solution algorithm for CVaR MDPs that enjoys error guarantees. Finally, we present results from numerical experiments that corroborate our theoretical findings and show the practicality of our approach.},
archivePrefix = {arXiv},
arxivId = {1506.02188},
author = {Chow, Yinlam and Tamar, Aviv and Mannor, Shie and Pavone, Marco},
eprint = {1506.02188},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/3Risk-sensitiveRobustDecisionMaking{\_}cvar.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1522--1530},
title = {{Risk-sensitive and robust decision-making: A CVaR optimization approach}},
volume = {2015-Janua},
year = {2015}
}
@article{Huber1964,
abstract = {This paper contains a new approach toward a theory of robust estimation; it treats in detail the asymptotic theory of estimating a location parameter for contaminated normal distributions, and exhibits estimators--intermediaries between sample mean and sample median--that are asymptotically most robust (in a sense to be specified) among all translation invariant estimators. For the general background, see Tukey (1960) (p. 448 ff.) Let x1,⋯,xnx{\_}1, $\backslash$cdots, x{\_}n be independent random variables with common distribution function F(t−$\xi$)F(t - $\backslash$xi). The problem is to estimate the location parameter $\xi$$\backslash$xi, but with the complication that the prototype distribution F(t)F(t) is only approximately known. I shall primarily be concerned with the model of indeterminacy F=(1−ϵ)$\Phi$+ϵHF = (1 - $\backslash$epsilon)$\backslash$Phi + $\backslash$epsilon H, where 0≦ϵ{\textless}10 $\backslash$leqq $\backslash$epsilon {\textless} 1 is a known number, $\Phi$(t)=(2$\pi$)−12∫t−∞exp(−12s2)ds$\backslash$Phi(t) = (2$\backslash$pi){\^{}}{\{}-$\backslash$frac{\{}1{\}}{\{}2{\}}{\}} $\backslash$int{\^{}}t{\_}{\{}-$\backslash$infty{\}} $\backslash$exp(-$\backslash$frac{\{}1{\}}{\{}2{\}}s{\^{}}2) ds is the standard normal cumulative and HH is an unknown contaminating distribution. This model arises for instance if the observations are assumed to be normal with variance 1, but a fraction ϵ$\backslash$epsilon of them is affected by gross errors. Later on, I shall also consider other models of indeterminacy, e.g., supt|F(t)−$\Phi$(t)|≦ϵ$\backslash$sup{\_}t |F(t) - $\backslash$Phi(t)| $\backslash$leqq $\backslash$epsilon. Some inconvenience is caused by the fact that location and scale parameters are not uniquely determined: in general, for fixed ϵ$\backslash$epsilon, there will be several values of $\xi$$\backslash$xi and $\sigma$$\backslash$sigma such that supt|F(t)−$\Phi$((t−$\xi$)/$\sigma$)|≦ϵ$\backslash$sup{\_}t|F(t) - $\backslash$Phi((t - $\backslash$xi)/$\backslash$sigma)| $\backslash$leqq $\backslash$epsilon, and similarly for the contaminated case. Although this inherent and unavoidable indeterminacy is small if ϵ$\backslash$epsilon is small and is rather irrelevant for practical purposes, it poses awkward problems for the theory, especially for optimality questions. To remove this difficulty, one may either (i) restrict attention to symmetric distributions, and estimate the location of the center of symmetry (this works for $\xi$$\backslash$xi but not for $\sigma$$\backslash$sigma); or (ii) one may define the parameter to be estimated in terms of the estimator itself, namely by its asymptotic value for sample size n→∞n $\backslash$rightarrow $\backslash$infty; or (iii) one may define the parameters by arbitrarily chosen functionals of the distribution (e.g., by the expectation, or the median of FF). All three possibilities have unsatisfactory aspects, and I shall usually choose the variant which is mathematically most convenient. It is interesting to look back to the very origin of the theory of estimation, namely to Gauss and his theory of least squares. Gauss was fully aware that his main reason for assuming an underlying normal distribution and a quadratic loss function was mathematical, i.e., computational, convenience. In later times, this was often forgotten, partly because of the central limit theorem. However, if one wants to be honest, the central limit theorem can at most explain why many distributions occurring in practice are approximately normal. The stress is on the word "approximately." This raises a question which could have been asked already by Gauss, but which was, as far as I know, only raised a few years ago (notably by Tukey): What happens if the true distribution deviates slightly from the assumed normal one? As is now well known, the sample mean then may have a catastrophically bad performance: seemingly quite mild deviations may already explode its variance. Tukey and others proposed several more robust substitutes--trimmed means, Winsorized means, etc.--and explored their performance for a few typical violations of normality. A general theory of robust estimation is still lacking; it is hoped that the present paper will furnish the first few steps toward such a theory. At the core of the method of least squares lies the idea to minimize the sum of the squared "errors," that is, to adjust the unknown parameters such that the sum of the squares of the differences between observed and computed values is minimized. In the simplest case, with which we are concerned here, namely the estimation of a location parameter, one has to minimize the expression ∑i(xi−T)2$\backslash$sum{\_}i (x{\_}i - T){\^{}}2; this is of course achieved by the sample mean T=∑ixi/nT = $\backslash$sum{\_}i x{\_}i/n. I should like to emphasize that no loss function is involved here; I am only describing how the least squares estimator is defined, and neither the underlying family of distributions nor the true value of the parameter to be estimated enters so far. It is quite natural to ask whether one can obtain more robustness by minimizing another function of the errors than the sum of their squares. We shall therefore concentrate our attention to estimators that can be defined by a minimum principle of the form (for a location parameter): T=Tn(x1,⋯,xn)minimizes∑i$\rho$(xi−T),T = T{\_}n(x{\_}1, $\backslash$cdots, x{\_}n) minimizes $\backslash$sum{\_}i $\backslash$rho(x{\_}i - T), where$\rho$isanon−constantfunction.(M)$\backslash$begin{\{}equation*{\}} $\backslash$tag{\{}M{\}} where $\backslash$rho is a non-constant function. $\backslash$end{\{}equation*{\}} Of course, this definition generalizes at once to more general least squares type problems, where several parameters have to be determined. This class of estimators contains in particular (i) the sample mean ($\rho$(t)=t2)($\backslash$rho(t) = t{\^{}}2), (ii) the sample median ($\rho$(t)=|t|)($\backslash$rho(t) = |t|), and more generally, (iii) all maximum likelihood estimators ($\rho$(t)=−logf(t)($\backslash$rho(t) = -$\backslash$log f(t), where ff is the assumed density of the untranslated distribution). These (MM)-estimators, as I shall call them for short, have rather pleasant asymptotic properties; sufficient conditions for asymptotic normality and an explicit expression for their asymptotic variance will be given. How should one judge the robustness of an estimator Tn(x)=Tn(x1,⋯,xn)T{\_}n(x) = T{\_}n(x{\_}1, $\backslash$cdots, x{\_}n)? Since ill effects from contamination are mainly felt for large sample sizes, it seems that one should primarily optimize large sample robustness properties. Therefore, a convenient measure of robustness for asymptotically normal estimators seems to be the supremum of the asymptotic variance (n→∞)(n $\backslash$rightarrow $\backslash$infty) when FF ranges over some suitable set of underlying distributions, in particular over the set of all F=(1−ϵ)$\Phi$+ϵHF = (1 - $\backslash$epsilon)$\backslash$Phi + $\backslash$epsilon H for fixed ϵ$\backslash$epsilon and symmetric HH. On second thought, it turns out that the asymptotic variance is not only easier to handle, but that even for moderate values of nn it is a better measure of performance than the actual variance, because (i) the actual variance of an estimator depends very much on the behavior of the tails of HH, and the supremum of the actual variance is infinite for any estimator whose value is always contained in the convex hull of the observations. (ii) If an estimator is asymptotically normal, then the important central part of its distribution and confidence intervals for moderate confidence levels can better be approximated in terms of the asymptotic variance than in terms of the actual variance. If we adopt this measure of robustness, and if we restrict attention to (MM)-estimators, then it will be shown that the most robust estimator is uniquely determined and corresponds to the following $\rho$:$\rho$(t)=12t2$\backslash$rho:$\backslash$rho(t) = $\backslash$frac{\{}1{\}}{\{}2{\}}t{\^{}}2 for |t|{\textless}k,$\rho$(t)=k|t|−12k2|t| {\textless} k, $\backslash$rho(t) = k|t| - $\backslash$frac{\{}1{\}}{\{}2{\}}k{\^{}}2 for |t|≧k|t| $\backslash$geqq k, with kk depending on ϵ$\backslash$epsilon. This estimator is most robust even among all translation invariant estimators. Sample mean (k=∞)(k = $\backslash$infty) and sample median (k=0)(k = 0) are limiting cases corresponding to ϵ=0$\backslash$epsilon = 0 and ϵ=1$\backslash$epsilon = 1, respectively, and the estimator is closely related and asymptotically equivalent to Winsorizing. I recall the definition of Winsorizing: assume that the observations have been ordered, x1≦x2≦⋯≦xnx{\_}1 $\backslash$leqq x{\_}2 $\backslash$leqq $\backslash$cdots $\backslash$leqq x{\_}n, then the statistic T=n−1(gxg+1+xg+1+xg+2+⋯+xn−h+hxn−h)T = n{\^{}}{\{}-1{\}}(gx{\_}{\{}g + 1{\}} + x{\_}{\{}g + 1{\}} + x{\_}{\{}g + 2{\}} + $\backslash$cdots + x{\_}{\{}n - h{\}} + hx{\_}{\{}n - h{\}}) is called the Winsorized mean, obtained by Winsorizing the gg leftmost and the hh rightmost observations. The above most robust (MM)-estimators can be described by the same formula, except that in the first and in the last summand, the factors xg+1x{\_}{\{}g + 1{\}} and xn−hx{\_}{\{}n - h{\}} have to be replaced by some numbers u,vu, v satisfying xg≦u≦xg+1x{\_}g $\backslash$leqq u $\backslash$leqq x{\_}{\{}g + 1{\}} and xn−h≦v≦xn−h+1x{\_}{\{}n - h{\}} $\backslash$leqq v $\backslash$leqq x{\_}{\{}n - h + 1{\}}, respectively; g,h,ug, h, u and vv depend on the sample. In fact, this (MM)-estimator is the maximum likelihood estimator corresponding to a unique least favorable distribution F0F{\_}0 with density f0(t)=(1−ϵ)(2$\pi$)−12e−$\rho$(t)f{\_}0(t) = (1 - $\backslash$epsilon)(2$\backslash$pi){\^{}}{\{}-$\backslash$frac{\{}1{\}}{\{}2{\}}{\}}e{\^{}}{\{}-$\backslash$rho(t){\}}. This f0f{\_}0 behaves like a normal density for small tt, like an exponential density for large tt. At least for me, this was rather surprising--I would have expected an f0f{\_}0 with much heavier tails. This result is a particular case of a more general one that can be stated roughly as follows: Assume that FF belongs to some convex set CC of distribution functions. Then the most robust (MM)-estimator for the set CC coincides with the maximum likelihood estimator for the unique F0$\epsilon$CF{\_}0 $\backslash$varepsilon C which has the smallest Fisher information number I(F)=∫(f′/f)2fdtI(F) = $\backslash$int (f'/f){\^{}}2f dt among all F$\epsilon$CF $\backslash$varepsilon C. Miscellaneous related problems will also be treated: the case of non-symmetric contaminating distributions; the most robust estimator for the model of indeterminacy supt|F(t)−$\Phi$(t)|≦ϵ$\backslash$sup{\_}t|F(t) - $\backslash$Phi(t)| $\backslash$leqq $\backslash$epsilon; robust estimation of a scale parameter; how to estimate location, if scale and ϵ$\backslash$epsilon are unknown; numerical computation of the estimators; more general estimators, e.g., minimizing ∑i{\textless}j$\rho$(xi−T,xj−T)$\backslash$sum{\_}{\{}i {\textless} j{\}} $\backslash$rho(x{\_}i - T, x{\_}j - T), where $\rho$$\backslash$rho is a function of two arguments. Questions of small sample size theory will not be touched in this paper.},
author = {Huber, Peter J.},
doi = {10.1214/aoms/1177703732},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
title = {{Robust Estimation of a Location Parameter}},
year = {1964}
}
@article{Ruszczynski2010,
abstract = {We introduce the concept of a Markov risk measure and we use it to formulate risk-averse control problems for two Markov decision models: a finite horizon model and a discounted infinite horizon model. For both models we derive risk-averse dynamic programming equations and a value iteration method. For the infinite horizon problem we develop a risk-averse policy iteration method and we prove its convergence. We also propose a version of the Newton method to solve a nonsmooth equation arising in the policy iteration method and we prove its global convergence. Finally, we discuss relations to min-max Markov decision models. {\textcopyright} 2010 Springer and Mathematical Optimization Society.},
author = {Ruszczy{\'{n}}ski, Andrzej},
doi = {10.1007/s10107-010-0393-3},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/Ruszczy{\'{n}}ski2010{\_}Article{\_}Risk-averseDynamicProgrammingF.pdf:pdf},
isbn = {1010701003933},
issn = {14364646},
journal = {Mathematical Programming},
keywords = {Dynamic risk measures,Markov risk measures,Min-max Markov models,Nonsmooth Newton's method,Policy iteration,Value iteration},
number = {2},
pages = {235--261},
title = {{Risk-averse dynamic programming for Markov decision processes}},
volume = {125},
year = {2010}
}
@inproceedings{Dabney2018b,
abstract = {In this work, we build on recent advances in distributional reinforcement learning to give a generally applicable, flexible, and state-of-the-art distributional variant of DQN. Wc achieve this by using quanlile regression to approximate the full quantile function for the state-action return distri-bution. By reparameterizing a distribution over the sample space, this yields an implicitly defined return distribution and gives rise to a large class of risk-sensitive policies. Wc demonstrate improved performance on the 57 Atari 2600 games in the ALE, and use our algorithm's implicitly defined distributions to study the effects of risk-sensitive policies in Atari games.},
archivePrefix = {arXiv},
arxivId = {1806.06923},
author = {Dabney, Will and Ostrovski, Georg and Silver, David and Munos, Remi},
booktitle = {35th International Conference on Machine Learning, ICML 2018},
eprint = {1806.06923},
isbn = {9781510867963},
title = {{Implicit quantile networks for distributional reinforcement learning}},
year = {2018}
}
@inbook{Bertsekas_nonlinear,
author = {Bertsekas, Dimitri},
title = {{Dynamic Programming and Optimal Control}},
volume = {1},
year = {1995}
}
@article{Rockafellar2000,
abstract = {A new approach to optimizing or hedging a portfolio of financial instruments to reduce risk is presented and tested on applications. It focuses on minimizing conditional value-at-risk (CVaR) rather than minimizing value-at-risk (VaR), but portfolios with low CVaR necessarily have low VaR as well. CVaR, also called mean excess loss, mean shortfall, or tail VaR, is in any case considered to be a more consistent measure of risk than VaR. Central to the new approach is a technique for portfolio optimization which calculates VaR and optimizes CVaR simultaneously. This technique is suitable for use by investment companies, brokerage firms, mutual funds, and any business that evaluates risk. It can be combined with analytical or scenario-based methods to optimize portfolios with large numbers of instruments, in which case the calculations often come down to linear programming or nonsmooth programming. The methodology can also be applied to the optimization of percentiles in contexts outside of finance.},
author = {Rockafellar, R. Tyrrell and Uryasev, Stanislav},
doi = {10.21314/jor.2000.038},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/important{\_}cvar{\_}read/2CVAR.pdf:pdf},
issn = {14651211},
journal = {The Journal of Risk},
number = {3},
pages = {21--41},
title = {{Optimization of conditional value-at-risk}},
volume = {2},
year = {2000}
}
@article{BarthMaron2018,
author = {Barth-Maron, Gabriel and Hoffman, Matthew W and Budden, David and Dabney, Will and Horgan, Dan and Dhruva, T B and Muldal, Alistair and Heess, Nicolas Manfred Otto and Lillicrap, Timothy P},
journal = {ArXiv},
title = {{Distributed Distributional Deterministic Policy Gradients}},
volume = {abs/1804.0},
year = {2018}
}
@article{Tamar2017,
abstract = {We provide sampling-based algorithms for optimization under a coherent-risk objective. The class of coherent-risk measures is widely accepted in finance and operations research, among other fields, and encompasses popular risk-measures such as conditional value at risk and mean-semi-deviation. Our approach is suitable for problems in which tuneable parameters control the distribution of the cost, such as in reinforcement learning or approximate dynamic programming with a parameterized policy. Such problems cannot be solved using previous approaches. We consider both static risk measures and time-consistent dynamic risk measures. For static risk measures, our approach is in the spirit of policy gradient methods, while for the dynamic risk measures, we use actor-critic type algorithms.},
author = {Tamar, Aviv and Chow, Yinlam and Ghavamzadeh, Mohammad and Mannor, Shie},
doi = {10.1109/TAC.2016.2644871},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/review{\_}cvar{\_}Friday31012013/Sequential Decision Making With Coherent Risk.pdf:pdf},
issn = {00189286},
journal = {IEEE Transactions on Automatic Control},
keywords = {Coherent risk,Markov decision processes,dynamic programming,policy gradient},
number = {7},
pages = {3323--3338},
publisher = {IEEE},
title = {{Sequential Decision Making with Coherent Risk}},
volume = {62},
year = {2017}
}
@inproceedings{Tamar2012,
abstract = {Managing risk in dynamic decision problems is of cardinal importance in many fields such as finance and process control. The most common approach to defining risk is through various variance related criteria such as the Sharpe Ratio or the standard deviation adjusted reward. It is known that optimizing many of the variance related risk criteria is NP-hard. In this paper we devise a framework for local policy gradient style algorithms for reinforcement learning for variance related criteria. Our starting point is a new formula for the variance of the cost-to-go in episodic tasks. Using this formula we develop policy gradient algorithms for criteria that involve both the expected cost and the variance of the cost. We prove the convergence of these algorithms to local minima and demonstrate their applicability in a portfolio planning problem. Copyright 2012 by the author(s)/owner(s).},
author = {Tamar, Aviv and {Di Castro}, Dotan and Mannor, Shie},
booktitle = {Proceedings of the 29th International Conference on Machine Learning, ICML 2012},
isbn = {9781450312851},
title = {{Policy gradients with variance related risk criteria}},
year = {2012}
}
@misc{Garcia2015,
abstract = {Safe Reinforcement Learning can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes. We categorize and analyze two approaches of Safe Reinforcement Learning. The first is based on the modification of the optimality criterion, the classic discounted finite/infinite horizon, with a safety factor. The second is based on the modification of the exploration process through the incorporation of external knowledge or the guidance of a risk metric. We use the proposed classification to survey the existing literature, as well as suggesting future directions for Safe Reinforcement Learning.},
author = {Garc{\'{i}}a, Javier and Fern{\'{a}}ndez, Fernando},
booktitle = {Journal of Machine Learning Research},
issn = {15337928},
keywords = {Reinforcement learning,Risk sensitivity,Safe exploration,Teacher advice},
title = {{A comprehensive survey on safe reinforcement learning}},
year = {2015}
}
@article{Morimura2010,
abstract = {Most conventional Reinforcement Learning (RL) algorithms aim to optimize decision-making rules in terms of the expected re-turns. However, especially for risk management purposes, other risk-sensitive criteria such as the value-at-risk or the expected shortfall are sometimes preferred in real applications. Here, we describe a parametric method for estimating density of the returns, which allows us to handle various criteria in a unified manner. We first extend the Bellman equation for the conditional expected return to cover a conditional probability density of the returns. Then we derive an extension of the TD-learning algorithm for estimating the return densities in an unknown environment. As test instances, several parametric density estimation algorithms are presented for the Gaussian, Laplace, and skewed Laplace distributions. We show that these algorithms lead to risk-sensitive as well as robust RL paradigms through numerical experiments.},
archivePrefix = {arXiv},
arxivId = {1203.3497},
author = {Morimura, Tetsuro and Sugiyama, Masashi and Kashima, Hisashi and Hachiya, Hirotaka and Tanaka, Toshiyuki},
eprint = {1203.3497},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/distributional{\_}RL/parametric{\_}return{\_}density{\_}estimation{\_}for{\_}RL.pdf:pdf},
isbn = {9780974903965},
journal = {Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence, UAI 2010},
pages = {368--375},
title = {{Parametric return density estimation for Reinforcement Learning}},
year = {2010}
}
@article{Wang2000,
abstract = {This article introduces a class of distortion operators, g(alpha)(u) =$\backslash$nPhi{\{}[{\}}Phi(-1)(u) + alpha], where Phi is the standard normal cumulative$\backslash$ndistribution. For any loss (or asset) variable X with a probability$\backslash$ndistribution S-x(x) = 1-F-x(x), g(alpha){\{}[{\}}S-x(x)] defines a distorted$\backslash$nprobability distribution whose mean value yields a risk-adjusted premium$\backslash$n(or an asset price). The distortion operator g(alpha) can be applied to$\backslash$nboth assets and liabilities, with opposite signs in the parameter alpha.$\backslash$nBased on CAPM, the author establishes that the parameter alpha should$\backslash$ncorrespond to the systematic risk of X. For a normal (mu, sigma(2))$\backslash$ndistribution, the distorted distribution is also normal with mu' = mu +$\backslash$nalpha sigma and sigma' = sigma. For a lognormal distribution, the$\backslash$ndistorted distribution is also lognormal. By applying the distortion$\backslash$noperator to stock price distributions, the author recovers the$\backslash$nrisk-neutral valuation for options and in particular the Black-Scholes$\backslash$nformula.},
author = {Wang, Shaun S.},
doi = {10.2307/253675},
issn = {00224367},
journal = {The Journal of Risk and Insurance},
title = {{A Class of Distortion Operators for Pricing Financial and Insurance Risks}},
year = {2000}
}
@article{Coraluppi1999,
abstract = {This paper analyzes a connection between risk-sensitive and minimax criteria for discrete-time, finite-state Markov decision processes (MDPs). We synthesize optimal policies with respect to both criteria, both for the finite horizon and the discounted infinite horizon problem. A generalized decision-making framework is introduced, which includes as special cases a number of approaches that have been considered in the literature. The framework allows for discounted risk-sensitive and minimax formulations leading to stationary optimal policies on the infinite horizon. We illustrate our results with a simple machine replacement problem. {\textcopyright} 1999 Elsevier Science Ltd. All rights reserved.},
author = {Coraluppi, Stefano and Marcus, Steven I.},
doi = {10.1016/S0005-1098(98)00153-8},
file = {:Users/nuriaarmengolurpi/ETH{\_}ZURICH/MASTER/Autumn2019/master{\_}thesis/papers/coraluppimarcus.pdf:pdf},
issn = {00051098},
journal = {Automatica},
keywords = {Markov decision processes,Minimax control,Risk-sensitive control,Stochastic Control},
number = {2},
pages = {301--309},
title = {{Risk-sensitive and minimax control of discrete-time, finite-state Markov decision processes}},
volume = {35},
year = {1999}
}
@misc{levine2020,
archivePrefix = {arXiv},
arxivId = {cs.LG/2005.01643},
author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
eprint = {2005.01643},
primaryClass = {cs.LG},
title = {{Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems}},
year = {2020}
}
@incollection{Majumdar2020,
abstract = {Endowing robots with the capability of assessing risk and making risk-aware decisions is widely considered a key step toward ensuring safety for robots operating under uncertainty. But, how should a robot quantify risk? A natural and common approach is to consider the framework whereby costs are assigned to stochastic outcomes - an assignment captured by a cost random variable. Quantifying risk then corresponds to evaluating a risk metric, i.e., a mapping from the cost random variable to a real number. Yet, the question of what constitutes a "good" risk metric has received little attention within the robotics community. The goal of this paper is to explore and partially address this question by advocating axioms that risk metrics in robotics applications should satisfy in order to be employed as rational assessments of risk. We discuss general representation theorems that precisely characterize the class of metrics that satisfy these axioms (referred to as distortion risk metrics), and provide instantiations that can be used in applications. We further discuss pitfalls of commonly used risk metrics in robotics, and discuss additional properties that one must consider in sequential decision making tasks. Our hope is that the ideas presented here will lead to a foundational framework for quantifying risk (and hence safety) in robotics applications.},
archivePrefix = {arXiv},
arxivId = {1710.11040},
author = {Majumdar, Anirudha and Pavone, Marco},
doi = {10.1007/978-3-030-28619-4_10},
eprint = {1710.11040},
title = {{How Should a Robot Assess Risk? Towards an Axiomatic Theory of Risk in Robotics}},
year = {2020}
}
@article{Hessel2018,
archivePrefix = {arXiv},
arxivId = {1710.02298},
author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Daniel and Piot, Bilal and Azar, Mohammad Gheshlaghi and Silver, David},
eprint = {1710.02298},
journal = {CoRR},
title = {{Rainbow: Combining Improvements in Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1710.02298},
volume = {abs/1710.0},
year = {2017}
}
@inproceedings{Fujimoto2018,
abstract = {In value-based reinforcement learning methods such as deep Q-leaming, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit over- estimation. We draw the conncction between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
archivePrefix = {arXiv},
arxivId = {1802.09477},
author = {Fujimoto, Scott and {Van Hoof}, Herke and Meger, David},
booktitle = {35th International Conference on Machine Learning, ICML 2018},
eprint = {1802.09477},
isbn = {9781510867963},
title = {{Addressing Function Approximation Error in Actor-Critic Methods}},
year = {2018}
}
@article{Dhaene2012,
abstract = {Distorted expectations can be expressed as weighted averages of quantiles. In this note, we show that this statement is essentially true, but that one has to be careful with the correct formulation of it. Furthermore, the proofs of the additivity property for distorted expectations of a comonotonic sum that appear in the literature often do not cover the case of a general distortion function. We present a straightforward proof for the general case, making use of the appropriate expressions for distorted expectations in terms of quantiles.},
author = {Dhaene, Jan and Kukush, Alexander and Linders, Dani{\"{e}}l and Tang, Qihe},
doi = {10.1007/s13385-012-0058-0},
issn = {21909741},
journal = {European Actuarial Journal},
keywords = {Comonotonicity,Distorted expectation,Distortion risk measure,Quantile,TVaR},
title = {{Remarks on quantiles and distortion risk measures}},
year = {2012}
}
@inproceedings{Geibel2006,
abstract = {In this article, I will consider Markov Decision Processes with two criteria, each defined as the expected value of an infinite horizon cumulative return. The second criterion is either itself subject to an inequality constraint, or there is maximum allowable probability that the single returns violate the constraint. I describe and discuss three new reinforcement learning approaches for solving such control problems.},
address = {Berlin, Heidelberg},
author = {Geibel, Peter},
booktitle = {Machine Learning: ECML 2006},
editor = {F{\"{u}}rnkranz, Johannes and Scheffer, Tobias and Spiliopoulou, Myra},
isbn = {978-3-540-46056-5},
pages = {646--653},
publisher = {Springer Berlin Heidelberg},
title = {{Reinforcement Learning for MDPs with Constraints}},
year = {2006}
}
@inproceedings{Taleghan2018,
author = {Taleghan, Majid Alkaee and Dietterich, Thomas G},
booktitle = {AAAI Spring Symposia},
title = {{Efficient Exploration for Constrained MDPs}},
year = {2018}
}
@book{koenker2005,
author = {Koenker, Roger},
doi = {10.1017/CBO9780511754098},
publisher = {Cambridge University Press},
series = {Econometric Society Monographs},
title = {{Quantile Regression}},
year = {2005}
}
@inproceedings{Lillicrap2016,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies “end-to-end”: directly from raw pixel inputs.},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
booktitle = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
eprint = {1509.02971},
title = {{Continuous control with deep reinforcement learning}},
year = {2016}
}
@inproceedings{Sohn2015,
abstract = {Supervised deep learning has been successfully applied to many recognition problems. Although it can approximate a complex many-to-one function well when a large amount of training data is provided, it is still challenging to model complex structured output representations that effectively perform probabilistic inference and make diverse predictions. In this work, we develop a deep conditional generative model for structured output prediction using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient variational Bayes, and allows for fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build robust structured prediction algorithms, such as input noise-injection and multi-scale prediction objective at training. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic inference. Furthermore, the proposed training methods are complimentary, which leads to strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
author = {Sohn, Kihyuk and Yan, Xinchen and Lee, Honglak},
booktitle = {Advances in Neural Information Processing Systems},
issn = {10495258},
title = {{Learning structured output representation using deep conditional generative models}},
year = {2015}
}
@article{Koenker2001,
author = {Koenker, Roger and Hallock, Kevin F.},
doi = {10.1257/jep.15.4.143},
issn = {08953309},
journal = {Journal of Economic Perspectives},
title = {{Quantile regression}},
year = {2001}
}
@inproceedings{Petrik2012,
abstract = {Stochastic domains often involve risk-averse decision makers. While recent work has focused on how to model risk in Markov decision processes using risk measures, it has not addressed the problem of solving large risk-averse formulations. In this paper, we propose and analyze a new method for solving large risk-averse MDPs with hybrid continuous-discrete state spaces and continuous action spaces. The proposed method iteratively improves a bound on the value function using a linearity structure of the MDP. We demonstrate the utility and properties of the method on a portfolio optimization problem.},
author = {Petrik, Marek and Subramanian, Dharmashankar},
booktitle = {Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012},
isbn = {9780974903989},
title = {{An approximate solution method for large risk-averse markov decision processes}},
year = {2012}
}
@article{Lagoudakis2004,
abstract = {We propose a new approach to reinforcement learning for control problems which combines value-function approximation with linear architectures and approximate policy iteration. This new approach is motivated by the least-squares temporal-difference learning algorithm (LSTD) for prediction problems, which is known for its efficient use of sample experiences compared to pure temporal-difference algorithms. Heretofore, LSTD has not had a straightforward application to control problems mainly because LSTD learns the state value function of a fixed policy which cannot be used for action selection and control without a model of the underlying process. Our new algorithm, least-squares policy iteration (LSPI), learns the state-action value function which allows for action selection without a model and for incremental policy improvement within a policy-iteration framework. LSPI is a model-free, off-policy method which can use efficiently (and reuse in each iteration) sample experiences collected in any manner. By separating the sample collection method, the choice of the linear approximation architecture, and the solution method, LSPI allows for focused attention on the distinct elements that contribute to practical reinforcement learning. LSPI is tested on the simple task of balancing an inverted pendulum and the harder task of balancing and riding a bicycle to a target location. In both cases, LSPI learns to control the pendulum or the bicycle by merely observing a relatively small number of trials where actions are selected randomly. LSPI is also compared against Q-learning (both with and without experience replay) using the same value function architecture. While LSPI achieves good performance fairly consistently on the difficult bicycle task, Q-learning variants were rarely able to balance for more than a small fraction of the time needed to reach the target location.},
author = {Lagoudakis, Michail G. and Parr, Ronald},
doi = {10.1162/1532443041827907},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Approximate Policy iteration,Least-Squares Methods,Markov Decision Processes,Reinforcement Learning,Value-Function Approximation},
title = {{Least-squares policy iteration}},
year = {2004}
}
@article{Vukobratovic2004ZeroMomentPoint,
author = {Vukobratovi{\'{c}}, Miomir and Borovac, Branislav},
issn = {0219-8436},
journal = {International Journal of Humanoid Robotics},
number = {01},
pages = {157--173},
publisher = {World Scientific},
title = {{Zero-moment point — thirty five years of its life}},
volume = {1},
year = {2004}
}
@inproceedings{Degris2012a,
abstract = {This paper presents the first actor-critic algorithm for off-policy reinforcement learning. Our algorithm is online and incremental, and its per-time-step complexity scales linearly with the number of learned weights. Previous work on actor-critic algorithms is limited to the on-policy setting and does not take advantage of the recent advances in off-policy gradient temporal-difference learning. Off-policy techniques, such as Greedy-GQ, enable a target policy to be learned while following and obtaining data from another (behavior) policy. For many problems, how-ever, actor-critic methods are more practical than action value methods (like Greedy-GQ) because they explicitly represent the policy; consequently, the policy can be stochastic and utilize a large action space. In this paper, we illustrate how to practically combine the generality and learning potential of off-policy learning with the flexibility in action selection given by actor-critic methods. We derive an incremental, linear time and space complexity algorithm that includes eligibility traces, prove convergence under assumptions similar to previous off-policy algorithms, and empirically show better or comparable performance to existing algorithms on standard reinforcement-learning benchmark problems. Copyright 2012 by the author(s)/owner(s).},
author = {Degris, Thomas and White, Martha and Sutton, Richard S.},
booktitle = {Proceedings of the 29th International Conference on Machine Learning, ICML 2012},
isbn = {9781450312851},
title = {{Off-policy actor-critic}},
year = {2012}
}
@article{Coraluppi1997,
author = {Coraluppi, Stefano P. and Marcus, Steven I.},
journal = {IEEE Transactions on Automatic Control},
number = {3},
pages = {528--532},
title = {{Mixed risk-neutral/minimax control of discrete-time, finite-state Markov decision processes}},
volume = {45},
year = {2000}
}
@article{Artzner1999,
abstract = {In this paper we study both market risks and nonmarket risks, without complete markets assumption, and discuss methods of measurement of these risks. We present and justify a set of four desirable properties for measures of risk, and call the measures satisfying these properties "coherent." We examine the measures of risk provided and the related actions required by SPAN, by the SEC/NASD rules, and by quantile-based methods. We demonstrate the universality of scenario-based methods for providing coherent measures. We offer suggestions concerning the SEC method. We also suggest a method to repair the failure of subadditivity of quantile-based methods.},
author = {Artzner, Philippe and Delbaen, Freddy and Eber, Jean Marc and Heath, David},
doi = {10.1111/1467-9965.00068},
issn = {09601627},
journal = {Mathematical Finance},
keywords = {Aggregation of risks,Butterfly,Capital requirement,Coherent risk measure,Concentration of risks,Currency risk,Decentralization,Extremal events risk,Insurance risk,Margin requirement,Market risk,Mean excess function,Measure of risk,Model risk},
title = {{Coherent measures of risk}},
year = {1999}
}
@article{Acerbi2002,
abstract = {Expected shortfall (ES) in several variants has been proposed as remedy for the deficiencies of value-at-risk (VaR) which in general is not a coherent risk measure. In fact, most definitions of ES lead to the same results when applied to continuous loss distributions. Differences may appear when the underlying loss distributions have discontinuities. In this case even the coherence property of ES can get lost unless one took care of the details in its definition. We compare some of the definitions of ES, pointing out that there is one which is robust in the sense of yielding a coherent risk measure regardless of the underlying distributions. Moreover, this ES can be estimated effectively even in cases where the usual estimators for VaR fail. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0104295},
author = {Acerbi, Carlo and Tasche, Dirk},
doi = {10.1016/S0378-4266(02)00283-2},
eprint = {0104295},
issn = {03784266},
journal = {Journal of Banking and Finance},
keywords = {Coherence,Conditional value-at-risk,Expected shortfall,Quantile,Risk measure,Sub-additivity,Tail conditional expectation,Tail mean,Value-at-risk,Worst conditional expectation},
primaryClass = {cond-mat},
title = {{On the coherence of expected shortfall}},
year = {2002}
}
@incollection{Feinberg2008,
abstract = {For single-criterion stochastic control and sequential decision problems, optimal policies, if they exist, are typically nonrandomized. For problems with multiple criteria and constraints, optimal nonrandomized policies may not exist and, if optimal policies exist, they are typically randomized. In this paper we discuss certain conditions that lead to optimality of nonrandomized policies. In the most interesting situations, these conditions do not impose convexity assumptions on the action sets and reward functions. {\textcopyright} 2008 Springer-Verlag Berlin Heidelberg.},
author = {Feinberg, Eugene A.},
booktitle = {Mathematical Control Theory and Finance},
doi = {10.1007/978-3-540-69532-5_8},
isbn = {9783540695318},
title = {{Optimality of deterministic policies for certain stochastic control problems with multiple criteria and constraints}},
year = {2008}
}
@article{Altman1993,
abstract = {We present in this paper several asymptotic properties of constrained Markov Decision Processes (MDPs) with a countable state space. We treat both the discounted and the expected average cost, with unbounded cost. We are interested in (1) the convergence of finite horizon MDPs to the infinite horizon MDP, (2) convergence of MDPs with a truncated state space to the problem with infinite state space, (3) convergence of MDPs as the discount factor goes to a limit. In all these cases we establish the convergence of optimal values and policies. Moreover, based on the optimal policy for the limiting problem, we construct policies which are almost optimal for the other (approximating) problems. Based on the convergence of MDPs with a truncated state space to the problem with infinite state space, we show that an optimal stationary policy exists such that the number of randomisations it uses is less or equal to the number of constraints plus one. We finally apply the results to a dynamic scheduling problem. {\textcopyright} 1993 Physica-Verlag.},
author = {Altman, Eitan},
doi = {10.1007/BF01414154},
issn = {03409422},
journal = {ZOR - Methods and Models of Operations Research},
keywords = {Constrained Markov Decision Processes,asymptotic properties,countable state space,dynamic scheduling,finite approximations,finite horizon,infinite horizon},
title = {{Asymptotic properties of constrained Markov Decision Processes}},
year = {1993}
}
@inproceedings{Kingma2014,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P. and Welling, Max},
booktitle = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
eprint = {1312.6114},
title = {{Auto-encoding variational bayes}},
year = {2014}
}
@article{Chow2014,
abstract = {In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in costs in addition to minimizing a standard criterion. Conditional value-at-risk (CVaR) is a relatively new risk measure that addresses some of the shortcomings of the well-known variance-related risk measures, and because of its computational efficiencies has gained popularity in finance and operations research. In this paper, we consider the mean-CVaR optimization problem in MDPs. We first derive a formula for computing the gradient of this risk-sensitive objective function. We then devise policy gradient and actor-critic algorithms that each uses a specific method to estimate this gradient and updates the policy parameters in the descent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in an optimal stopping problem.},
archivePrefix = {arXiv},
arxivId = {1406.3339},
author = {Chow, Yinlam and Ghavamzadeh, Mohammad},
eprint = {1406.3339},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {January},
pages = {3509--3517},
title = {{Algorithms for CVaR optimization in MDPs}},
volume = {4},
year = {2014}
}
@inproceedings{Pratt1995SEA,
author = {Pratt, Gill A and Williamson, Matthew M},
booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.1995.525827},
pages = {3137--3181},
title = {{Series elastic actuators}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=525827},
year = {1995}
}
@article{Uhlenbeck1930,
abstract = {With a method first indicated by Ornstein the mean values of all the powers of the velocity u and the displacement s of a free particle in Brownian motion are calculated. It is shown that u-u0exp(-$\beta$t) and s-u0$\beta$[1-exp(-$\beta$t)] where u0 is the initial velocity and $\beta$ the friction coefficient divided by the mass of the particle, follow the normal Gaussian distribution law. For s this gives the exact frequency distribution corresponding to the exact formula for s2 of Ornstein and F{\"{u}}rth. Discussion is given of the connection with the Fokker-Planck partial differential equation. By the same method exact expressions are obtained for the square of the deviation of a harmonically bound particle in Brownian motion as a function of the time and the initial deviation. Here the periodic, aperiodic and overdamped cases have to be treated separately. In the last case, when $\beta$ is much larger than the frequency and for values of t$\beta$-1, the formula takes the form of that previously given by Smoluchowski. {\textcopyright} 1930 The American Physical Society.},
author = {Uhlenbeck, G. E. and Ornstein, L. S.},
doi = {10.1103/PhysRev.36.823},
issn = {0031899X},
journal = {Physical Review},
title = {{On the theory of the Brownian motion}},
year = {1930}
}
