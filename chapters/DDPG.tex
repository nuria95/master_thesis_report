\chapter{The algorithm }
\label{chapter:algo}

We introduce an off-policy, model-free algorithm for CVaR optimization using deep function approximators
that can learn policies in high-dimensional, continuous action spaces.
Our work is based on the deterministic policy gradient algorithm.\\
Specifically, we use an actor-critic approach: the critic uses a distributional variant
of RL and it is trained to estimate the whole value distribution, whereas the actor
is trained via gradient ascent to maximize the CVaR of this distribution.\\ 
In the following we briefly describe the standard DPG algorithm \ref{sec:DPG}, then we introduce
our distributional approach and how we use it to obtain risk-sensitive policies \ref{sec:distr_ddpg}
and in the last section, we focus more on implementation details of the algorithm, 
especially explaining more about the neural networks we use and providing a
pseudocode \ref{sec:technical_details}.

Extensive and more theoretical information on the distributional RL approach is addressed in 
chapter \ref{chapter:distr_rl} / appendix \ref{chap:appendix} \todo{decide}

\section{Off-policy Deterministic Actor-Critic} \label{sec:DPG} \todo{change title not to repeat with subsection?}
\subsection{Preliminaries}

Goal in standard RL is to learn a policy $\pi^*$ which maximizes the expected return or (discounted) cumulative
reward $R$ collected by the agent when acting in an environment $E$ starting from any initial state $x$.
Action-value function for policy $\pi$, $Q^\pi(x,a)$, is used in many RL algorithms and describes the expected return after taking
an action $a$ in state $x$, and thereafter following policy $\pi$:
\begin{equation}
    Q^\pi(x_t,a_t) = \mathbb E_{r_{i\geq t},x_{i>t} \sim E, a_{i>t}\sim \pi}\Big[  \sum_{i=t}^T \gamma^{(i-t)}r(x_i,a_i) \Big]
\end{equation}

Bellman's equation describes this value Q using the 
recursive relationship between the action-value of a state and the action-values of its
successor states:
\begin{equation}
    Q^\pi(x_t,a_t) = \mathbb E_{r_t,x_{t+1} \sim E}\Big[ r(x_t,a_t)] + \gamma \mathbb E_{a_{t+1}\sim \pi}\big[Q^\pi(x_{t+1},a_{t+1})\big]\Big] \label{eq:bellman1}
\end{equation}


DPG algorithm \citep{silver2014b} is characterized for using deterministic policies $a_t=\mu_{\theta}(x_t)$.\\
In general, behaving according to a deterministic policy does not ensure adequate exploration
and may lead to suboptimal solutions. 
However, if the policy is deterministic, we can remove the inner expectation with respect 
to $a_{t+1}$ in \ref{eq:bellman1} and then, crucially, the expected cumulative reward in the next-state 
depends only on the environment and not on the policy distribution used to create the samples.
\begin{equation}
    Q^\pi(x_t,a_t) = \mathbb E_{r_t,x_{t+1} \sim E}\Big[ r(x_t,a_t) + \gamma Q^\pi(x_{t+1},\pi(x_{t+1}))\Big]
\end{equation}
\label{par:offpolicy}
This means that it is possible to learn the value function
$Q^\pi$ off-policy, i.e. using environment interactions which are generated by acting
under a different stochastic
behavior policy $\beta$ (where $\beta \neq \pi$) which ensures enough exploration.
An advantage of off-policy algorithms is that we can treat the problem of exploration
independently from the learning algorithm.

To learn the optimal policy, Q-learning \cite{Watkins1992}, a commonly used off-policy algorithm, first learns the optimal 
value function $Q^*$ by iteratively applying the Bellman optimality operator to the current Q estimate:
\begin{equation}
    Q(x_t,a_t) \leftarrow \mathbb E_{r_t,x_{t+1} \sim E}\Big[ r(x_t,a_t) + \gamma \underset{a_{t+1}} \max Q(x_{t+1},a_{t+1})\Big]
\end{equation}
which is a contraction mapping proved to converge exponentially to $Q^*$, and then
derives the optimal policy $\pi^*$from it via the greedy policy $a^*=\underset{a}{\text{argmax}} Q^*(x,a)$.

When dealing with continuous actions, it is not possible to apply Q-learning
straight-forward because finding the greedy policy requires an optimization of $a$ at 
every timestep, which is too slow to be practical with large action spaces.
In this case, policy gradient methods are used in which a \textit{parameterized policy} is learnt 
to be able to select actions without consulting the value function.


\subsection{Deterministic policy gradients}
The deterministic policy gradient described by \citet{silver2014b} updates the parameters of the
deterministic policy $\pi_\theta$ via gradient ascent to maximize an objective function $J(\pi_\theta)$:

\begin{align}
    J(\pi_\theta) &=  \mathbb E_{x \sim \rho^\pi} [Z(x,\pi_\theta(x))]\\
    \nabla_\theta J(\pi_\theta) &=  \mathbb E_{x \sim \rho^\pi} 
    \big [\nabla_{\theta} \pi_\theta(x) \nabla_a Q^{\pi}(x,a)|_{a=\pi_\theta(x)}  \big]
\end{align} 
where $\rho^\pi$ is the discounted state distribution when acting under policy $\pi$.


\subsection{Off-policy Deterministic actor-critic} \label{subsec:offpolicy_actor_critic}
When we both learn approximations of Q and policy, the method is called deterministic actor-critic.
The actor is the learned policy which is updated with respect to the current value estimate, or critic.
\cite{Sutton1998}.

In the off-policy setting, the critic parameterized by $\theta^Q$ estimates the 
action-value function $Q^{\pi}(x,a)$ off-policy  from
trajectories generated by a behavior policy $\beta$ (discussed in \ref{par:offpolicy}) 
using the Bellman equation.
It learns by minimizing the loss:

\begin{align}
    \mathcal{L}(\theta^Q) = \mathbb E_{x_t\sim \rho^\beta, a_t\sim \beta, r_t\sim E} \Big[ (Q(x_t,a_t| \theta^Q)-y_t)^2          \Big] \\
    y_t = r(x_t,a_t) + \gamma Q(x_{t+1},\pi(x_{t+1})| \theta^Q)
\end{align}

The actor, parameterized by $\theta^\pi$, updates its parameters via gradient ascent by
using the off-policy deterministic policy gradient \citet{silver2014b}:
\begin{align}
    J_\beta(\pi | \theta^\pi) &= \int_\mathcal{X} \rho^\beta(s)Q^{\pi}(x,\pi (x| \theta^\pi))dx \nonumber\\
    \nabla_{\theta^\pi} J_\beta(\pi | \theta^\pi) &\approx \mathbb E_{x \sim \rho^\beta} 
    \big [\nabla_{\theta^\pi} \pi(x,| \theta^\pi) \nabla_a Q^{\pi}(x,a)|_{a=\pi(x| \theta^\pi)}  \big] \label[]{eq:off_policy_gradient}
\end{align} 
where $\rho^\beta$ is the discounted state distribution when acting under behavior policy $\beta$.
By propagating the gradient through both policy and Q, the actor learns an approximation to the
maximum of the value function under target policy $\pi$
averaged over the state distribution of the behavior policy $\beta$.

A term that depends on $\nabla_{\theta^\pi} Q^{\pi}(x,a)$ has been 
dropped in \ref{eq:off_policy_gradient}, following a justification
given by \cite{Degris2012} that argues that this is a good approximation since it can
preserve the set of local optima to which gradient ascent converges.


\section{Distributional off-policydeterministic actor critic} \label{sec:distr_ddpg}
We present the CVaR optimization algorithm which is the main contribution of this thesis.
The algorithm is based on the original off-policy deterministic actor critic explained
in previous subsection \ref{subsec:offpolicy_actor_critic}, but introduces a distributional
critic, which estimates the whole value distribution instead of only its expected value.
With this extra information, the actor can learn to maximize other metrics than the expected value,
specifically the CVaR.
We proceed to present the components of the algorithm.


\subsection{Distributional Critic}
We use a distributional variant of the standard critic function,
which maps from state-action pairs to distributions, inspired by the implicit quantile network (IQN) introduced in \cite{Dabney2018b}.\\
IQN is a deterministic parametric function trained to reparameterize samples from a
base distribution, e.g $\tau \in U([0,1])$, to the respective
quantile values of a target distribution.\\
We define $Z(x,a)$ as the random variable representing the return, with cumulative 
distribution function $F(z):=P(Z\leq z)$ and we define $F^{-1}_Z(\tau):=Z(x,a;\tau)$ as its quantile function 
(or inverse cumulative distribution function)
at $\tau \in [0,1]$.
Thus, for $\tau \in U([0,1])$, the resulting state-action return distribution sample is
$Z(x,a;\tau)\sim Z(x,a)$.\\
The critic IQN network $Z(x,a;\tau| \theta^Z)$ parameterized by $\theta^Z$  is hence a parametric function
used to represent the quantile function at specific quantile levels.

As in \citet{Dabney2018b},we train the IQN critic network using the sampled quantile regression
loss \citep{koenker2005} on the pairwise temporal-difference (TD)-errors.
For two samples $\tau, \tau' \sim U([0,1])$, and current policy $\pi_{\theta^\pi}$, the sampled
TD error is:

\begin{align}
    \delta^{\tau, \tau'} = r + \gamma Z(x_{t+1},\pi(x_{t+1}|\theta^\pi);\tau'| \theta^Z)-Z(x_t,a_t;\tau|\theta^Z)
\end{align}
Then, we compute the loss over the quantile samples:
\begin{equation}
    \mathcal{L}_{QR}(x_t,a_t,r_t,x_{t+1})= \frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{N'}\rho_{\tau_i}^\kappa(\delta^{\tau_i, \tau_j'}) \label[]{eq:qr_loss}
\end{equation}
where N and N' are the number of iid samples $\tau_i, \tau_j' \sim U([0,1]$ used to estimate the loss and
where $\rho^\kappa_\tau$ is the quantile Huber loss.\\
Quantile Huber loss acts as an assymetric squared loss in an interval $[-\kappa, \kappa]$ around zero
and reverts to a standard quantile loss outside this interval.\\
The Huber loss is given by \citet{Huber1964}:

\begin{equation}
    \mathcal{L}_\kappa(u)  = \left\{
	    \begin{array}{ll}
		 \frac{1}{2}u^2      & \mathrm{if\ } |u| \le \kappa \\
		 \kappa(|u|-\frac{1}{2}\kappa)    & \mathrm{otherwise }
	    \end{array}
	     \right.
\end{equation}

Then the quantile Huber loss is the assymetric variant of the Huber loss:

\begin{align}
    \rho_\tau^\kappa(u)=\Big|\tau - [u<0]\Big|\mathcal{L}_\kappa(u) , \forall u \in \mathbb{R}
\end{align}
where $[\cdot]$ is the Iverson bracket.
Quantile loss penalizes overestimation errors (u$<$0) with weight $1-\tau$ and underestimation
errors (u$>$0)with weight $\tau$.
By minimizing \ref{eq:qr_loss} via stochastic gradient descent with respect to $\theta^Z$
we aim to move towards the true quantile function. \todo{Second, can the contraction
mapping results for a fixed grid of quantiles given by Dabney et al. (2018) be extended to
the more general class of approximate quantile functions
studied in this work?}

\subsection{Actor}
The policy is updated via deterministic policy gradient ascent.
We modify equation  \eqref{eq:off_policy_gradient}, to include the action-value distribution.

\begin{align}
    \nabla_{\theta^\pi} J_\beta(\pi | \theta^\pi) &\approx \mathbb E_{x \sim \rho^\beta} 
    \big [\nabla_{\theta^\pi} \pi(x,| \theta^\pi) \nabla_a Q^{\pi}(x,a)|_{a=\pi(x| \theta^\pi)}  \big]\label{eq:distr_dpg_alg1}\\
    &=\mathbb E_{x \sim \rho^\beta} 
    \big [\nabla_{\theta^\pi} \pi(x,| \theta^\pi) \mathbb E [\nabla_a Z^\pi(x,a | \theta^Z)]|_{a=\pi(x| \theta^\pi)}  \big]
    \label{eq:distr_dpg_alg2}
\end{align}


Step from \eqref{eq:distr_dpg_alg1} to \eqref{eq:distr_dpg_alg2} comes by the fact that
\begin{equation}
    Q^\pi(x,a) = \mathbb E[Z^\pi(x,a)] \label{eq:neutral_policy}
\end{equation}

With our goal of CVaR optimization in mind:

\begin{equation}
     \arg \underset{\pi}\max \text{CVaR}_\alpha [Z (x, \pi(x))] \quad \forall x \in \mathcal{X}
\end{equation}
we can make use of the information provided by the Z distribution to 
optimize other objective functions rather than the expected value.

To approach this, we use as a performance objective 
the distorted expectation of Z(x,a) under the distortion risk measure $\phi: [0,1] \ra [0,1]$, with
identity corresponding to risk-neutrality, i.e.:
\begin{equation}
    Q_\phi(x,a) = \mathbb E_{\tau \sim U([0,1])} [Z_{\phi(\tau)}(x,a)] \label{eq:risk_policy}
\end{equation}
which is actually equivalent to the expected value of $F^{-1}_{Z(x,a)}$ weighted by $\phi$, i.e.:
\begin{equation}
    Q_\phi(x,a) = \int_0^1F^{-1}_{Z}(\tau)d\phi(\tau) \label[]{eq:cvar_intuition}
\end{equation}
When we use as mapping $\phi(\tau) = \alpha \tau$, \ref{eq:cvar_intuition} corresponds to the 
 CVaR of Z(x,a) as already presented in \ref{eq:cvar_defs}, and as a reminder:
\begin{equation}
    \text{CVaR}_\alpha (Z) =  \frac{1}{\alpha} \int_{0}^{\alpha} F^{-1}_Z(\tau) d\tau \label{eq:cvar_defs_repeat}
 \end{equation}

Again, \ref{eq:cvar_defs_repeat} is the Acerbi's integral formula for CVaR, which
states that CVaR at confidence level $\alpha$ can be interpreted as the integral of all the quantiles
below the corresponding $\alpha$.
We can hence approximate \eqref{eq:cvar_defs_repeat} via sampling, by taking $K$ samples of 
$\tau \sim U[0,\alpha]$:
\begin{equation}
\text{CVaR}_\alpha (Z) \approx \frac{1}{\alpha} \frac{1}{K}\sum_{i=1}^KZ(x,a; \tau_i)
\qquad \tau_i \sim U[0,\alpha] \quad \forall i \in [1,K]
\end{equation}

Therefore, we use arrive to the formula for the \textit{deterministic risk-sensitive policy-gradient},
the following \textit{distorted policy gradient} for the actor network: \todo{call it like that?}

\begin{align}
    J_\beta^{CVAR}(\pi | \theta^\pi) &= \int_\mathcal{X} \rho^\beta(s)\text{CVaR}_\alpha(Z^\pi(x,\pi (x| \theta^\pi))dx \nonumber\\
    \nabla_{\theta^\pi} J_\beta(\pi | \theta^\pi) &\approx \mathbb E_{x \sim \rho^\beta} 
    \big [\nabla_{\theta^\pi} \pi(x,| \theta^\pi) \nabla_a  [\frac{1}{\alpha} \frac{1}{K}
    \sum_{i=1}^K Z(x,a; \tau_i) | \theta^Z)]|_{a=\pi(x| \theta^\pi)}  \big]
    \label{eq:actor_grad}
\end{align}
where $\tau_i \in U([0,\alpha]) \; \forall i \in [1,K]$\\


The algorithm can be then summed up via:

\begin{enumerate}
    \item Update distributional critic network via off-policy quantile-regression TD-learning:
    \begin{align}
        \delta^{\tau, \tau'} = r + \gamma Z(x_{t+1},\pi(x_{t+1}|\theta^\pi);\tau'| \theta^Z)-Z(x_t,a_t;\tau|\theta^Z) \nonumber
    \end{align}
    \item Update actor network via deterministic risk-sensitive policy gradient ascent:
    \begin{align}
        \nabla_{\theta^\pi} J_\beta(\pi | \theta^\pi) &\approx \mathbb E_{x \sim \rho^\beta} 
    \big [\nabla_{\theta^\pi} \pi(x,| \theta^\pi) \nabla_a  [\frac{1}{\alpha} \frac{1}{K}
    \sum_{i=1}^K Z(x,a; \tau_i) | \theta^Z)]|_{a=\pi(x| \theta^\pi)}  \big] \nonumber
    \end{align}
\end{enumerate}

Having its main two steps above, we again remark the capability of the algorithm to be
implemented off-policy. We act in the environment using 
aa more-exploratory behavior policy $\beta$ and we learn a
target policy $\pi$ (where $\beta \neq \pi$).
In contrast to stochastic off-policy actor-critic algorithms \citep{Degris2012}, we can avoid
importance sampling both in the actor and the critic.
This is due to the deterministic essence of the policies, which removes the integral over actions
in the policy gradient and the expected value over actions in the Bellman equation for the critic.
