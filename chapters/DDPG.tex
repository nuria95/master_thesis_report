\chapter{The \texorpdfstring{$\thename$} {algorithm}}
\label{chapter:algo}

We introduce the $\thename$ algorithm (Distributional Deterministic Actor-Critic algorithm for
CVaR optimization), an off-policy, model-free algorithm for CVaR optimization using deep function approximators
that can learn policies in high-dimensional, continuous action spaces.\\
The word \textit{distributional} emphasizes that our approach takes inspiration from the
recent advances in distributional RL 
\citep{Bellemare2017,Dabney2018a,Dabney2018b} to estimate the full
\textit{action-value distribution}.\\
Additionally, we build upon DDPG \citep{Lillicrap2016}, a popular off-policy actor-critic 
using deep RL method for continuous control.

Specifically, our critic uses the distributional variant
of RL and it is trained to learn a parameterization of the action-value distribution under current policy.
Our actor maximizes the CVaR of this distribution
by sampling from the parameterized value distribution to compute the CVaR and
being updated using a variant of the
deterministic policy gradient algorithm.

In \ref{sec:DPG} we briefly describe the standard DPG algorithm and its variants,
then in \ref{sec:distr_ddpg} we introduce
the $\thename$ algorithm and how we use it to obtain risk-sensitive policies,
in \ref{sec:technical_details} we focus on implementation details of the algorithm and we provide
a pseudocode \ref{pseudocode:algo1} and in \ref{sec:other_cvar_computations} and \ref{sec:other_risk_measures} we present
other alternatives for computing CVaR and other possible risk metrics that could be optimized without varying
the algorithm.
Extensive and more theoretical information on the distributional RL approach is addressed in 
appendix \ref{chap:appendix_distRL}.

\section{Deterministic Policy Gradients} \label{sec:DPG}
\subsection{Preliminaries}

The goal in standard RL is to learn a policy $\pi^*$ which maximizes the expected return or (discounted) cumulative
reward sampled from reward distribution $R(x,a)$ collected by the agent when acting in 
an environment with transition probability distribution $P(\cdot| x_{t},a_t)$ starting
from any initial state $x$.
The action-value function for policy $\pi$, $Q^\pi(x,a)$, is used in many RL algorithms and describes the expected return after taking
an action $a$ in state $x$, and thereafter following policy $\pi$:
\begin{equation}
    Q^\pi(x_t,a_t) = \mathbb E_{r_{i\geq t}\sim R,x_{i>t} \sim P, a_{i>t}\sim \pi}\Big[  \sum_{i=t}^T \gamma^{(i-t)}r(x_i,a_i) \Big]
\end{equation}

The Bellman's equation describes this value Q using the 
recursive relationship between the action-value of a state and the action-values of its
successor states:
\begin{equation}
    Q^\pi(x_t,a_t) = \mathbb E_{r_t\sim R,x_{t+1} \sim P}\Big[ r(x_t,a_t)] + \gamma \mathbb E_{a_{t+1}\sim \pi}\big[Q^\pi(x_{t+1},a_{t+1})\big]\Big] \label{eq:bellman1}
\end{equation}

The DPG algorithm \citep{silver2014b} is characterized for using deterministic policies $a_t=\mu_{\theta}(x_t)$.\\
In general, behaving according to a deterministic policy does not ensure adequate exploration
and may lead to suboptimal solutions. 
However, if the policy is deterministic we can remove the inner expectation with respect 
to $a_{t+1}$ in \ref{eq:bellman1} and then crucially, the expected cumulative reward in the next-state 
depends only on the environment and not on the policy distribution used to create the samples.
\begin{equation}
    Q^\pi(x_t,a_t) = \mathbb E_{r_t \sim R,x_{t+1} \sim P}\Big[ r(x_t,a_t) + \gamma Q^\pi(x_{t+1},\pi(x_{t+1}))\Big]
\end{equation}
\label{par:offpolicy}
This means that it is possible to learn the value function
$Q^\pi$ off-policy, i.e. using environment interactions which are generated by acting
under a different stochastic
behavior policy $\beta$ (where $\beta \neq \pi$) which ensures enough exploration.
An advantage of off-policy algorithms is that we can treat the problem of exploration
independently from the learning algorithm.

To learn the optimal policy, Q-learning \cite{Watkins1992}, a commonly used off-policy algorithm, first learns the optimal 
value function $Q^*$ by iteratively applying the Bellman optimality operator to the current Q estimate:
\begin{equation}
    Q(x_t,a_t) \leftarrow \mathbb E_{r_t \sim R,x_{t+1} \sim P}\Big[ r(x_t,a_t) + \gamma \underset{a_{t+1}} \max Q(x_{t+1},a_{t+1})\Big]
\end{equation}
which is a contraction mapping proved to converge exponentially to $Q^*$, and then
derives the optimal policy $\pi^*$ from it via the greedy policy $a^*=\underset{a}{\text{argmax}} Q^*(x,a)$.

When dealing with continuous actions, it is not possible to apply Q-learning
straight-forward because finding the greedy policy requires an optimization of $a$ at 
every timestep, which is too slow to be practical with large action spaces.
In this case, policy gradient methods are used in which a parameterized policy is learnt 
alongside the Q-function to be able to select actions without consulting the value function.

\subsection{The Deterministic policy gradient}
The deterministic policy gradient described by \citet{silver2014b} updates the parameters of the
deterministic policy $\pi_\theta$ via gradient ascent to maximize an objective function $J(\pi_\theta)$:
\begin{align}
    J(\pi_\theta) &=  \mathbb E_{x \sim \rho^\pi} [Z(x,\pi_\theta(x))]\\
    \nabla_\theta J(\pi_\theta) &=  \mathbb E_{x \sim \rho^\pi} 
    \big [\nabla_{\theta} \pi_\theta(x) \nabla_a Q^{\pi}(x,a)|_{a=\pi_\theta(x)}  \big]
\end{align} 
where $\rho^\pi$ is the discounted state distribution when acting under policy $\pi$.


\subsection{Off-policy Deterministic actor-critic} \label{subsec:offpolicy_actor_critic}
When we both learn approximations of the policy and the value function, the method is called deterministic actor-critic.
Actor-critic algorithms combine the basic ideas from policy gradients and approximate dynamic programming.
The actor is the learned policy which is updated with respect to the current value estimate, or critic.
\cite{Sutton1998}.

Actor-critic methods are closely related with policy iteration methods \citep{Lagoudakis2004}
which consists of two phases: policy evaluation and policy improvement. \\
Unlike Q-learning, which directly attempts to learn the optimal Q-function, policy evaluation
phase computes the Q-function for the current policy $\pi$, $Q^\pi$, by solving
for the fixed point such that $Q^\pi=\mathcal{T}^\pi Q^\pi$.
In the off-policy setting, as discussed previously, the critic (parameterized by $\theta^Q$), can estimate the 
action-value function $Q^{\pi}(x,a)$ \textit{off-policy}  from
trajectories generated by a behavior policy $\beta$
using the Bellman equation.
It learns by minimizing the loss:
\begin{align}
    \mathcal{L}(\theta^Q) = \mathbb E_{x_t\sim \rho^\beta, a_t\sim \beta} \Big[ (Q(x_t,a_t| \theta^Q)-y_t)^2          \Big] \nonumber\\ 
    y_t = r(x_t,a_t) + \gamma Q(x_{t+1},\pi(x_{t+1})| \theta^Q) \label{eq:bellman_loss}
\end{align}

The policy improvement phase is done by choosing the action that greedily maximizes the Q-value at
each state.
With this aim, the actor (parameterized by $\theta^\pi$) updates its parameters via gradient ascent by
using the off-policy deterministic policy gradient \citet{silver2014b}:
\begin{align}
    J_\beta(\pi | \theta^\pi) &= \int_\mathcal{X} \rho^\beta(s)Q^{\pi}(x,\pi (x| \theta^\pi))dx \nonumber\\
    \nabla_{\theta^\pi} J_\beta(\pi | \theta^\pi) &\approx \mathbb E_{x \sim \rho^\beta} 
    \big [\nabla_{\theta^\pi} \pi(x,| \theta^\pi) \nabla_a Q^{\pi}(x,a)|_{a=\pi(x| \theta^\pi)}  \big] \label[]{eq:off_policy_gradient}
\end{align} 
where $\rho^\beta$ is the discounted state distribution when acting under behavior policy $\beta$.
By propagating the gradient through both policy and Q, the actor learns an approximation to the
maximum of the value function under policy $\pi$
averaged over the state distribution of the behavior policy $\beta$.\\
A term that depends on $\nabla_{\theta^\pi} Q^{\pi}(x,a)$ has been 
dropped in \ref{eq:off_policy_gradient}, following a justification
given by \cite{Degris2012} that argues that this is a good approximation since it can
preserve the set of local optima to which gradient ascent converges.

\section{The \texorpdfstring{$\thename$} {algorithm}} \label{sec:distr_ddpg}
We present the $\thename$ algorithm, which is one of the main contributions of this thesis.
The algorithm is based on the original off-policy deterministic actor critic explained
in previous subsection \ref{subsec:offpolicy_actor_critic}, but introduces a distributional
critic which estimates the whole value distribution instead of only its expected value.
With this extra information, the actor can learn to maximize other metrics than the expected value,
specifically the CVaR.
We proceed to present the components of the algorithm.


\subsection{Distributional Critic}
We use a distributional variant of the standard critic function
which maps from state-action pairs to distributions, inspired by the implicit quantile network (IQN) introduced in \cite{Dabney2018b}.\\
IQN is a deterministic parametric function trained to reparameterize samples from a
base distribution, e.g $\tau \in U([0,1])$, to the respective
quantile values of a target distribution.\\
We define $Z(x,a)$ as the random variable representing the return, with cumulative 
distribution function $F(z):=P(Z\leq z)$ and we define $F^{-1}_Z(\tau):=Z(x,a;\tau)$ as its quantile function 
(or inverse cumulative distribution function)
at $\tau \in [0,1]$.
Thus, for $\tau \in U([0,1])$, the resulting state-action return distribution sample is
$Z(x,a;\tau)\sim Z(x,a)$.\\
The IQN critic $Z(x,a;\tau)$ parameterized by $\theta^Z$  is hence a parametric function
used to represent the quantile function at specific quantile levels.

As in \citet{Dabney2018b},we train the IQN critic using the sampled quantile regression
loss \citep{koenker2005} on the pairwise temporal-difference (TD)-errors.\\
Quantile regression $\rho_{\tau}(u)$ is a method for approximating quantile functions of a distribution at specific points.
Quantile regression loss for quantile $\tau \in [0,1]$ is an asymmetric convex lox function
that penalizes underestimation errors (u$<$0) with weight $1-\tau$ and underestimation
errors (u$>$0) with weight $\tau$. See figure \ref{fig:quantile_loss} for a graphical representation. 

For two samples $\tau, \tau' \sim U([0,1])$, current policy $\pi$ (parameterized by $\theta^\pi$) and
replay buffer tuple $(x_{t},a_t,x_{t+1},r_t)$
the sampled TD error is:
\begin{align}
    \delta^{\tau, \tau'} = r_t + \gamma Z(x_{t+1},\pi(x_{t+1});\tau')-Z(x_t,a_t;\tau)
\end{align}
Then, we compute the loss over the quantile samples:
\begin{equation}
    \mathcal{L}_{QR}(x_t,a_t,r_t,x_{t+1})= \frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{N'}\rho_{\tau_i}^\kappa(\delta^{\tau_i, \tau_j'}) \label[]{eq:qr_loss}
\end{equation}
where N and N' are the number of iid samples $\tau_i, \tau_j' \sim U([0,1]$ used to estimate the loss and
where $\rho^\kappa_\tau$ is the quantile Huber loss.\\
Quantile Huber loss acts as an asymmetric squared loss in an interval $[-\kappa, \kappa]$ around zero
and reverts to a standard quantile loss outside this interval.\\
The Huber loss \citep{Huber1964} is given by:
\begin{equation}
    \mathcal{L}_\kappa(u)  = \left\{
	    \begin{array}{ll}
		 \frac{1}{2}u^2      & \mathrm{if\ } |u| \le \kappa \\
		 \kappa(|u|-\frac{1}{2}\kappa)    & \mathrm{otherwise }
	    \end{array}
	     \right.
\end{equation}

Then the quantile Huber loss is the asymmetric variant of the Huber loss:

\begin{align}
    \rho_\tau^\kappa(u)=\Big|\tau - \mathds{1}_{\{u<0\}}\Big|\mathcal{L}_\kappa(u) , \forall u \in \mathbb{R}
\end{align}

The loss \ref{eq:qr_loss} gives unbiased sample gradients and hence we can
minimize it with respect to $\theta^Z$ by stochastic gradient descent. Doing so, we 
find the parameters $\theta^Z$ to estimate the true quantile function.\\
For additional information
and reasoning on the approach we address the reader to the appendix section \ref{sec:appendix_quantile_approximation}.\\

\subsection{Actor}
We update the policy via deterministic policy gradient ascent.
We modify equation \eqref{eq:off_policy_gradient}, to include the action-value distribution.
\begin{align}
    \nabla_{\theta^\pi} J_\beta(\pi | \theta^\pi) &\approx \mathbb E_{x \sim \rho^\beta} 
    \big [\nabla_{\theta^\pi} \pi(x,| \theta^\pi) \nabla_a Q^{\pi}(x,a)|_{a=\pi(x| \theta^\pi)}  \big]\label{eq:distr_dpg_alg1}\\
    &=\mathbb E_{x \sim \rho^\beta} 
    \big [\nabla_{\theta^\pi} \pi(x,| \theta^\pi) \mathbb E [\nabla_a Z^\pi(x,a | \theta^Z)]|_{a=\pi(x| \theta^\pi)}  \big]
    \label{eq:distr_dpg_alg2}
\end{align}


The step from \eqref{eq:distr_dpg_alg1} to \eqref{eq:distr_dpg_alg2} comes by the fact that
\begin{equation}
    Q^\pi(x,a) = \mathbb E[Z^\pi(x,a)] \label{eq:neutral_policy}
\end{equation}

With our goal of CVaR optimization in mind:
\begin{equation}
     \arg \underset{\pi}\max \text{CVaR}_\alpha [Z (x, \pi(x))] \quad \forall x \in \mathcal{X}
\end{equation}
we can make use of the information provided by the Z distribution to 
optimize other objective functions rather than the expected value.\\
To approach this, we use as a performance objective 
the distorted expectation of Z(x,a) under the distortion risk measure $\phi: [0,1] \ra [0,1]$, with
identity corresponding to risk-neutrality, i.e.:
\begin{equation}
    Q_\phi(x,a) = \mathbb E_{\tau \sim U([0,1])} [Z_{\phi(\tau)}(x,a)] \label{eq:risk_policy}
\end{equation}
which is equivalent to the expected value of $F^{-1}_{Z(x,a)}$ weighted by $\phi$, i.e.:
\begin{equation}
    Q_\phi(x,a) = \int_0^1F^{-1}_{Z}(\tau)d\phi(\tau) \label[]{eq:cvar_intuition}
\end{equation}
When we use $\phi(\tau) = \alpha \tau$ as a mapping, \ref{eq:cvar_intuition} corresponds to the 
 CVaR of Z(x,a) as already presented in \ref{eq:cvar_defs}, and as a reminder:
\begin{equation}
    \text{CVaR}_\alpha (Z) =  \frac{1}{\alpha} \int_{0}^{\alpha} F^{-1}_Z(\tau) d\tau \label{eq:cvar_defs_repeat}
 \end{equation}

Again, \ref{eq:cvar_defs_repeat} is the Acerbi's integral formula for CVaR, which
states that CVaR at confidence level $\alpha$ can be interpreted as the integral of all the quantiles
below the corresponding $\alpha$.
We can hence approximate \eqref{eq:cvar_defs_repeat} via sampling, by taking $K$ samples of 
$\tau \sim U[0,\alpha]$:
\begin{equation}
\text{CVaR}_\alpha (Z) \approx \frac{1}{\alpha} \frac{1}{K}\sum_{i=1}^KZ(x,a; \tau_i)
\qquad \tau_i \sim U[0,\alpha] \quad \forall i \in [1,K]
\end{equation}

Therefore, we arrive to the formula for the $\thename$ actor parameters $\theta^\pi$ update:
\begin{align}
    J_\beta^{CVAR}(\pi) &= \int_\mathcal{X} \rho^\beta(x)\text{CVaR}_\alpha\Big(Z^\pi(x,\pi (x))\Big)dx \nonumber\\
    \nabla_{\theta^\pi} J_\beta(\pi) &\approx \mathbb E_{x \sim \rho^\beta} 
    \big [\nabla_{\theta^\pi} \pi(x) \nabla_a  [\frac{1}{\alpha} \frac{1}{K}
    \sum_{i=1}^K Z(x,a; \tau_i)]|_{a=\pi(x)}  \big]
    \label{eq:actor_grad}
\end{align}
where $\tau_i \in U([0,\alpha]) \; \forall i \in [1,K]$\\


\subsection{Summary}
The algorithm can be summed up with:

\begin{enumerate}
    \item Update distributional critic IQN via off-policy quantile-regression TD-learning:
    \begin{align}
        \delta^{\tau, \tau'} = r + \gamma Z(x_{t+1},\pi(x_{t+1});\tau')-Z(x_t,a_t;\tau) \label{eq:alg1}
    \end{align}
    \item Update actor via deterministic risk-sensitive policy gradient ascent:
    \begin{align}
        \nabla_{\theta^\pi} J_\beta(\pi ) &\approx \mathbb E_{x \sim \rho^\beta} 
    \big [\nabla_{\theta^\pi} \pi(x) \nabla_a  [\frac{1}{\alpha} \frac{1}{K}
    \sum_{i=1}^K Z(x,a; \tau_i)]|_{a=\pi(x)}  \big] \label{eq:alg2}
    \end{align}
\end{enumerate}

Having the two main steps above, we again remark the capability of the algorithm to be
implemented off-policy. We act in the environment using 
a more-exploratory behavior policy $\beta$ and we learn a
target policy $\pi$ (where $\beta \neq \pi$).
In contrast to stochastic off-policy actor-critic algorithms \citep{Degris2012}, we can avoid
importance sampling both in the actor and the critic.
This is thanks to the deterministic essence of the policies, which removes the integral over actions
in the policy gradient for the actor and removes the expected value over actions in the Bellman equation
for the critic.
