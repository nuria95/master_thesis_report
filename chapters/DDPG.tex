\chapter{The algorithm }
\label{chapter:algo}

We introduce an off-policy, model-free algorithm for CVaR optimization using deep function approximators
that can learn policies in high-dimensional, continuous action spaces.
Our work is based on the deterministic policy gradient algorithm.\\
Specifically, we use an actor-critic approach: the critic uses a distributional variant
of RL and it is trained to estimate the whole value distribution, whereas the actor
is trained via gradient ascent to maximize the CVaR of this distribution.\\ 
In the following, we briefly describe the standard DPG algorithm \ref{sec:DPG}, then we introduce
our distributional approach and the way we move towards risk-sensitive policies.
Extensive and more theoretical information on the distributional RL approach is addressed in 
chapter \ref{chapter:distr_rl}

\section{Off-policy Deterministic Actor-Critic} \label{sec:DPG} \todo{change title not to repeat with subsection?}
\subsection{Preliminaries}

Goal in standard RL is to learn a policy $\pi^*$ which maximizes the expected return or (discounted) cumulative
reward $R$ collected by the agent when acting in an environment $E$ starting from any initial state $x$.
Action-value function is used in many RL algorithms and describes the expected return after taking
an action $a$ in state $x$, and thereafter following policy $\pi$:
\begin{equation}
    Q^\pi(x_t,a_t) = \mathbb E_{r_{i\geq t},x_{i>t} \sim E, a_{i>t}\sim \pi}\Big[  \sum_{i=t}^T \gamma^{(i-t)}r(x_i,a_i) \Big]
\end{equation}

Bellman's equation describes this value Q using the 
recursive relationship between the action-value of a state and the action-values of its
successor states:
\begin{equation}
    Q^\pi(x_t,a_t) = \mathbb E_{r_t,x_{t+1} \sim E}\Big[ r(x_t,a_t)] + \gamma \mathbb E_{a_{t+1}\sim \pi}\big[Q^\pi(x_{t+1},a_{t+1})\big]\Big] \label{eq:bellman1}
\end{equation}


DPG algorithm \citep{silver2014b} is characterized for using deterministic policies $a_t=\mu_{\theta}(x_t)$.

In general, behaving according to a deterministic policy does not ensure adequate exploration
and may lead to suboptimal solutions. 
However, if the policy is deterministic, the expected cumulative reward in the next-state 
depends only on the environment, hence we can remove the inner expectation in \ref{eq:bellman1} to:
\begin{equation}
    Q^\pi(x_t,a_t) = \mathbb E_{r_t,x_{t+1} \sim E}\Big[ r(x_t,a_t) + \gamma Q^\pi(x_{t+1},\pi(x_{t+1}))\Big]
\end{equation}
\label{par:offpolicy}This means that it is possible to learn the value function
$Q^\pi$ off-policy, i.e. using environment interactions which are generated by acting under a different stochastic
behavior policy $\beta$ (where $\beta \neq \pi$) which ensures enough exploration.
An advantage of off-policy algorithms is that we can treat the problem of exploration
independently from the learning algorithm.

To learn the optimal policy, Q-learning \cite{Watkins1992}, a commonly used off-policy algorithm, first learns the optimal 
value function $Q^*$ by iteratively applying the Bellman optimality operator to the current Q estimate:
\begin{equation}
    Q(x_t,a_t) \leftarrow \mathbb E_{r_t,x_{t+1} \sim E}\Big[ r(x_t,a_t) + \gamma \underset{a_{t+1}} \max Q(x_{t+1},a_{t+1})\Big]
\end{equation}
which is a contraction mapping proved to converge exponentially to $Q^*$, and then
derives the optimal policy $\pi^*$from it via the greedy policy $a^*=\underset{a}{\text{argmax}} Q^*(x,a)$.

When dealing with continuous actions, it is not possible to apply Q-learning
straight-forward because finding the greedy policy requires an optimization of $a$ at 
every timestep, which is too slow to be practical with large action spaces.
In this case, policy gradient methods are used in which a \textit{parameterized policy} is learnt 
to be able to select actions without consulting the value function.


\subsection{Deterministic policy gradients}
The deterministic policy gradient described by \citet{silver2014b} updates the parameters of the
deterministic policy $\pi_\theta$ via gradient ascent to maximize an objective function $J(\pi_\theta)$:

\begin{align}
    J(\pi_\theta) &=  \mathbb E_{x \sim \rho^\pi} [Z(x,\pi_\theta(x))]\\
    \nabla_\theta J(\pi_\theta) &=  \mathbb E_{x \sim \rho^\pi} 
    \big [\nabla_{\theta} \pi_\theta(x) \nabla_a Q^{\pi}(x,a)|_{a=\pi_\theta(x)}  \big]
\end{align} 
where $\rho^\pi$ is the discounted state distribution when acting under policy $\pi$.


\subsection{Off-policy Deterministic actor-critic}
When we both learn approximations of Q and policy, the method is called deterministic actor-critic.
The actor is the learned policy which is updated with respect to the current value estimate, or critic.
\cite{Sutton1998}.

In the off-policy setting, the critic parameterized by $\theta^Q$ estimates the 
action-value function $Q^{\pi}(x,a)$ off-policy  from
trajectories generated by a behavior policy $\beta$ (argumented in \ref{par:offpolicy}) 
using the Bellman equation.
It learns by minimizing the loss:

\begin{align}
    \mathcal{L}(\theta^Q) = \mathbb E_{x_t\sim \rho^\beta, a_t\sim \beta, r_t\sim E} \Big[ (Q(x_t,a_t| \theta^Q)-y_t)^2          \Big] \\
    y_t = r(x_t,a_t) + \gamma Q(x_{t+1},\pi(x_{t+1})| \theta^Q)
\end{align}

The actor, parameterized by $\theta^\pi$, learns via gradient ascent by using the off-policy deterministic policy
gradient, which was proved
by \cite{silver2014b}  to be the policy gradient, i.e. the gradient of the policy's performance:
\begin{align}
    J_\beta(\pi | \theta^\pi) &= \int_\mathcal{X} \rho^\beta(s)Q^{\pi}(x,\pi (x| \theta^\pi))dx \nonumber\\
    \nabla_{\theta^\pi} J_\beta(\pi | \theta^\pi) &\approx \mathbb E_{x \sim \rho^\beta} 
    \big [\nabla_{\theta^\pi} \pi(x,| \theta^\pi) \nabla_a Q^{\pi}(x,a)|_{a=\pi(x| \theta^\pi)}  \big] \label[]{eq:off_policy_gradient}
\end{align} 
where $\rho^\beta$ is the discounted state distribution when acting under behavior policy $\beta$.
By propagating the gradient through both policy and Q, the actor learns an approximation to the
maximum of the value function under target policy $\pi$
averaged over the state distribution of the behavior policy $\beta$.

A term that depends on $\nabla_\theta Q^{\mu_\theta}(x,a) $ has been dropped in \, following a justification
given by \cite{Degris2012} that argues that this is a good approximation since it can
preserve the set of local optima to which gradient ascent converges.


Similarly to \cite{Lillicrap2016}, we will use neural networks as non-linear function approximators
for learning both action-value functions and the deterministic target policy.





\section{Distributional off-policy deterministic actor critic}
We present the CVaR optimization algorithm which is the main contribution of this thesis.
The algorithm is based on the original off-policy deterministic actor critic explained
in previous section \ref{sec:DPG}, but introduces a distributional
critic, which estimates the whole value distribution instead of only its expected value.
With this extra information, the actor can learn to maximize other metrics than the expected value,
specifically the CVaR.
We proceed to present the components of the algorithm:


\subsection{Distributional Critic}
We use a distributional variant of the standard critic function,
which maps from state-action pairs to distributions, similar
to the implicit quantile network (IQN) introduced in \cite{Dabney2018b}.\\
IQN is a deterministic parametric function trained to reparameterize samples from a
base distribution, e.g $\tau \in U([0,1])$, to the respective
quantile values of a target distribution.\\
We define $Z(x,a)$ as the random variable representing the return, with cumulative 
distribution function $F(z):=P(Z\leq z)$ and we define $F^{-1}_Z(\tau):=Z(x,a;\tau)$ as its quantile function 
(or inverse cumulative distribution function)
at $\tau \in [0,1]$.
Thus, for $\tau \in U([0,1])$, the resulting state-action return distribution sample is
$Z(x,a;\tau)\sim Z(x,a)$.\\
The critic IQN network $Z(x,a;\tau| \theta^Z)$ parameterized by $\theta^Z$,  is hence a parametric function
used to represent the quantile function at specific quantile levels.

As in \citet{Dabney2018b},we train the IQN network using the sampled quantile regression
loss \citep{koenker2005} on the pairwise temporal-difference (TD)-errors.
For two samples $\tau, \tau' \sim U([0,1])$, and current policy $\pi_{\theta^\pi}$, the sampled
TD error is:

\begin{align}
    \delta^{\tau, \tau'} = r + \gamma Z(x_{t+1},\pi(x_{t+1}|\theta^\pi);\tau'| \theta^Z)-Z(x_t,a_t;\tau|\theta^Z)
\end{align}
Then, we compute the quantile regression loss:
\begin{equation}
    \mathcal{L}_{QR}(x_t,a_t,r_t,x_{t+1})= \frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{N'}\rho_{\tau_i}(\delta^{\tau_i, \tau_j'}) \label[]{eq:qr_loss}
\end{equation}

where $\rho$ is as defined as:
\begin{align}
    \rho_\tau(u)=u(\tau - \delta_{u<0}) , \forall u \in \mathbb{R} \nonumber
\end{align}
where N and N' are the number of iid samples $\tau_i, \tau_j' \sim U([0,1]$ used to estimate the loss.\\
By minimizing \ref{eq:qr_loss} via stochastic gradient descent with respect to $\theta^Z$
we move towards the true quantile function \citep{koenker2005}.

\subsection{Actor}
The policy is updated via deterministic policy gradient ascent.
We modify equation  \eqref{eq:off_policy_gradient}, to include the action-value distribution.

\begin{align}
    \nabla_{\theta^\pi} J_\beta(\pi | \theta^\pi) &\approx \mathbb E_{x \sim \rho^\beta} 
    \big [\nabla_{\theta^\pi} \pi(x,| \theta^\pi) \nabla_a Q^{\pi}(x,a)|_{a=\pi(x| \theta^\pi)}  \big]\label{eq:distr_dpg_alg1}\\
    &=\mathbb E_{x \sim \rho^\beta} 
    \big [\nabla_{\theta^\pi} \pi(x,| \theta^\pi) \mathbb E [\nabla_a Z^\pi(x,a | \theta^Z)]|_{a=\pi(x| \theta^\pi)}  \big]
    \label{eq:distr_dpg_alg2}
\end{align}


Step from \eqref{eq:distr_dpg_alg1} to \eqref{eq:distr_dpg_alg2} comes by the fact that
\begin{equation}
    Q^\pi(x,a) = \mathbb E[Z^\pi(x,a)] \label{eq:neutral_policy}
\end{equation}

With our goal of CVaR optimization in mind:

\begin{equation}
     \arg \underset{\pi}\max \text{CVaR}_\alpha [Z (x, \pi(x))] \quad \forall x \in \mathcal{X}
\end{equation}
we can make use of the information provided by the Z distribution to 
optimize other objective functions rather than the expected value.

To approach this, we use as a performance objective 
the distorted expectation of Z(x,a) under the distortion risk measure $\phi: [0,1] \ra [0,1]$, with
identity corresponding to risk-neutrality, i.e.:
\begin{equation}
    Q_\phi(x,a) = \mathbb E_{\tau \sim U([0,1])} [Z_{\phi(\tau)}(x,a)] \label{eq:risk_policy}
\end{equation}
which is actually equivalent to the expected value of $F^{-1}_{Z(x,a)}$ weighted by $\phi$, i.e.:
\begin{equation}
    Q_\phi(x,a) = \int_0^1F^{-1}_{Z}(\tau)d\phi(\tau) \label[]{eq:cvar_intuition}
\end{equation}
When we use as mapping $\phi(\tau) = \alpha \tau$, \ref{eq:cvar_intuition} is actually the CVaR of Z(x,a) as
already presented in \ref{eq:cvar_defs}, and as a reminder:
\begin{equation}
    \text{CVaR}_\alpha (Z) =  \frac{1}{\alpha} \int_{0}^{\alpha} F^{-1}_Z(\tau) d\tau \label{eq:cvar_defs_repeat}
 \end{equation}


We can hence approximate \eqref{eq:cvar_defs_repeat} via sampling, by taking $K$ samples of 
$\tau \sim U[0,\alpha]$:
\begin{equation}
\text{CVaR}_\alpha (Z) \approx \frac{1}{\alpha} \frac{1}{K}\sum_{i=1}^KZ(x,a; \tau_i)
\qquad \tau_i \sim U[0,\alpha] \quad \forall i \in [1,K]
\end{equation}

Finally, then, we use the following \textit{distorted policy gradient} for the actor network: \todo{call it like that?}

\begin{align}
    J_\beta^{CVAR}(\pi | \theta^\pi) &= \int_\mathcal{X} \rho^\beta(s)\text{CVaR}_\alpha(Z^\pi(x,\pi (x| \theta^\pi))dx \nonumber\\
    \nabla_{\theta^\pi} J_\beta(\pi | \theta^\pi) &\approx \mathbb E_{x \sim \rho^\beta} 
    \big [\nabla_{\theta^\pi} \pi(x,| \theta^\pi) \nabla_a  [\frac{1}{\alpha} \frac{1}{k}
    \sum_{i=1}^kZ(x,a; \tau_i) | \theta^Z)]|_{a=\pi(x| \theta^\pi)}  \big]
    \label{eq:actor_grad}
\end{align}
where $\tau_i \in U([0,\alpha]) \; \forall i$\\

We again remark the capability of the algorithm to be implemented off-policy. We use 
a more-exploratory behavior policy $\beta$ to learn a target policy $\pi$ (where $\beta \neq \pi$) and,
in contrast to stochastic off-policy actor-critic algorithms \citep{Degris2012}, we can avoid
importance sampling both in the actor and the critic.
This is due to the deterministic essence of the policies, which removes the integral over actions
in the policy gradient and the expected value over actions in the Bellman equation for the critic.

\section{Technical Details of the algorithm}
s

\begin{itemize}
    \item Explain replay buffer characteristics
    \item Target networks and update frequencies
    \item Adam optimizers (p4 lillicrap)
	\item Ornstein Noises and exponential decays
	\item Add pseudo-code of the algorithm 
\end{itemize}


From a practical viewpoint, using stochastic policies requires integrating over both
state and action spaces to compute the policy gradient, whereas the deterministic case
only needs to integrate over the state space. Hence, stochastic policy gradients
may require much more samples, especially if the action space has many dimensions.



use replay buffer, target networks as DQN to deal with problems of Qlearning with functions approximators.

Since distorted expectations can be expressed as weighted average over the quantiles \cite{Dhaene2012},
we can use a specific sampling base distribution $\beta: [0,1] \ra [0,1]$
to sample the quantile levels $\tau \in[0,1])$ from our critic network $Z(x,a; \tau)$.