\chapter{Batch RL}\label{chapter:batchrl}

We decide to test the capabilities of our algorithm in a \textit{fully} off-policy setting, 
also called \textit{batch RL setting} or \textit{offline RL}. In this setting, the agent 
can only learn from a fixed dataset without further interaction with the environment.

Most of RL algorithms provide a fundamentally \textit{online} learning paradigm which involves
iteratively collecting experience by interacting with the environment, and then
using that experience to improve the policy. 
In many  real-world scenarios, continuous interacting with the environment is impractical, either because 
data collection is expensive, as in robotics or healthcare, or dangerous, for example a physical robot may
get its hardware damaged or damage surrounding objects.
Additionally, even in some cases were online interaction is feasible, we might prefer to still use previously
collected datasets - for example, if the domain is complex (such as a robot learning to cook several meals at home)
and effective generalization (e.g cooking in different kitchens or using different ingredients for the meal)
requires large datasets \cite{levine2020}.


The ``off-policy" algorithm we presented in previous sections, and similarly to most of the ``off-policy"
algorithms in literature, falls in the category of 
off-policy \textit{``growing batch learning"}. In this case, agent's experience is appended to a data buffer 
$\mathcal{D}$ and each new policy $\pi_k$ collects additional data, such that $\mathcal{D}$
is composed of samples from $\pi_0,\pi_1...,\pi_k$ and all of this data is used to train an update new
policy $\pi_{k+1}$. 
%As a result, the training data tends to be heavily correlated to the current policy.
Hence, despite being off-policy these methods require active online data collection.

In contrast, in offline RL the agent no longer has ability to interact with the environment and collect
additional transitions. Instead, it
can only employ a static dataset $\mathcal{D}$ collected by an external agent using policy $\pi_\beta$
and must learn the best policy it can only using this dataset.

In numerous real-world applications, such as games or robotics, there is already plentiful amounts 
of previously collected interaction data which are a rich source of prior information.
RL algorithms that can train agents using these prior datasets without further data collection
will not only scale to real-world problems, but will also lead to solutions that generalize substantially better.
Off-line RL holds tremendous promise for making it possible to turn large datasets into powerful 
decision-making engines, effectively allowing anyone with a large enough dataset to turn this dataset
into a policy than can optimize a desired utility criterion. \citep{levine2020}.


\section{Issues with Batch RL} \todo{change title?}

Most of recent off-policy algorithms such as Soft Actor-Critic \citep{Haarnoja2018}, 
DDPG \citep{Lillicrap2016} and Rainbow \citep{Hessel2018}  could be in principle used 
as an offline RL algorithm, i.e. learn from data collected from an unknown behavioral policy
$\pi_\beta$ with state visitation frequency $d^{\pi_\beta}(s)$. 
However, in practice, they still require substantial amounts
of “on-policy” data from the current behavioral policy in order to learn effectively, 
and generally they fail to learn in the off-line setting.

This is due to a fundamental problem of off-policy RL, called \textit{distributional shift}, which in 
its turn induces what's called extrapolation error \citep{Fujimoto2019} or bootstrapping error \citet{Kumar2019}.\\
Distributional shift affects offline RL via dynamic programming algorithms (as ours, i.e. methods aiming
to learn a state or state-action value function), both at test time and training time.
State distribution shift affects test-time performance, but no training, since neither the policy nor
the Q-function is ever evaluated at any state that was not sampled from $d^{\pi_\beta}$.\\
However, training process is indeed affected by \textit{action distribution shift}.
This is because when doing the Bellman backup to update the Q-function estimate,
the \textit{target} values require evaluating the estimated $Q^\pi(x_{k+1},a_{k+1})$ where
a is chosen according to current policy $\pi$.
Since targets are computed via bootstrapping, accuracy of Q-function regression depends
on the estimate of the Q-value at these actions. When $\pi$ differs substantially from $\pi_\beta$, 
these actions are generally unlikely or not contained in the dataset, i.e.
out of the distribution of actions that Q-function was ever trained on. This can result
in highly erroneous targets Q-values, and in its turn, in updated new Q estimates 
with pathological values that incur large absolute error from the optimal desired Q-value.\\
This issue is further exacerbated when $\pi$ is explicitly optimized to maximize $Q^\pi(s,\pi(a))$.
Then, the policy will learn to produce out-of-distribution (OOD) actions
for which the learnt Q-function erroneously overestimates.
This source of distribution shift is in fact one of the largest obstacles for practical application
of dynamic programming methods to offline RL.

It is important to notice that for on-policy settings, extrapolation error is generally something positive, since it
leads to a beneficial exploration. In this case, if the value function is overestimating the value at
a (state-action) pair, the current policy will lead the agent to that pair and will observe that in fact
it is not as good and hence, the value estimate will be corrected afterwards.
However, in the offline setting, the correction step on such over-optimistic Q-values cannot be done by the 
policy, due to the inability of collecting new data, and these errors accumulate over each iteration of training,
resulting in arbitrarily poor final results or even divergence.


To address this OOD actions problems and effectively implement offline RL via dynamic programming,
\citet{Fujimoto2018} opted for constraining the trained policy distribution to lie close to
the dataset policy distribution, to avoid erroneous Q-values due to extrapolation error.
\citet{Kumar2019} suggested a less restrictive solution to constrain the learnt policy to lie
\textit{within the support} of the dataset policy distribution.
\todo{maybe cite more?}

For our algorithm, we will inspire ourselves in the approach presented in 
\citet{Fujimoto2019}, which we introduce below with its further modifications.


\section{Our approach} \todo{more explicit title?}
In \citet{Fujimoto2019}, they present a Batch-Constrained Deep Q-learning (BCQ)
algorithm in which they
train a generative model $G_w$ to generate actions with high similarity to the dataset.
For the generative model they use a conditional variational auto-encoder (VAE) \cite{Kingma2014} which
generates action samples as a reasonable approximation to
$\underset{a}{\text{argmax}}  P _\mathcal{B}^G(a|s)$, where $P_\mathcal{B}^G(a|s)$ is 
the state-conditioned marginal likelihood given the (state-action) pairs in the 
batch $\mathcal{B}$.

\subsection{Details of VAE}

A variational autoencoder aims to maximize the marginal log-likelihood $\log p(X) = \sum_{i=1}^{N}\log p(x_i)$
, where X is the dataset with iid samples $\{x_{1},x_2,...x_N\}$.
It is assumed that data is generated by some random process, involving an unobserved continuous
random variable \textbf{z}.
The process consists of two steps: (1) a value \textbf{$z_i$} is generated from some prior distribution p(z) and (2)
a value $x_i$ is generated from some conditional distribution $p(x|z)$.
Given that the true probabilities are unknown, a recognition model $q(z|x; \phi )$ is introduced
as an approximation to the intractable true posterior $p(z|x; \theta)$.

The recognition model $q(z|x; \phi)$ is called an \textit{encoder},  since given a datapoint x it produces
a \textit{random latent vector} z.
$p(x|z; \theta)$ is called a \textit{decoder},
since given the random latent vector z it reconstructs the original sample x.

Since computing the desired marginal $p(X; \theta)$ is intractable, VAE algorithm optimizes a lower bound instead:

\begin{equation}
    \log p(X; \theta) \geq \mathcal{L}(\theta, \phi; X) = \mathbb E_{q(z|X;\phi)} [\log p(X|z; \theta)] - D_{KL}(q(z|X;\phi)\; ||\;p(z; \theta))
\end{equation}

For our implementation, the prior $p(z; \theta)$ is chosen to be a multivariate normal
distribution $\mathcal{N}\big( 0,Id\big )$, hence it lacks parameters.

For the probabilistic encoders and decoders we use neural networks.
For the encoder $q(z|x_i;\phi)$ we used a neural network with  Gaussian output, specifically a 
multivariate Gaussian with a diagonal covariance structure $\mathcal{N}(z | \mu(X), \sigma^2(X)Id)$,
where $\mu$ and $\sigma$ are the outputs of the neural network, i.e. nonlinear 
functions of datapoint $x_i:=(state_i,action_i)$ and $\phi$.
To sample from the posterior $z_i \sim q(z|x_i; \phi)$ we use the reparameterization trick:
$z_i = g(x_i, \epsilon; \phi)=\mu_i + \sigma_i \odot \epsilon$ where $\epsilon \sim  \mathcal{N}(0,Id)$
and $\odot$ is the element-wise product.
For the decoder $p(x|z; \theta)$ we used another neural network with deterministic output, i.e. nonlinear function of 
datapoint $\hat x_i:=(state_i,z_i)$ and $\theta$.


The VAE is trained to maximize  reconstruction loss and a KL-divergence term according to the distribution of
the latent vector:

When it comes to training the VAE, both recognition model parameters $\phi$ and the generative model parameters $\theta$ are
learnt jointly to maximize the variational lower bound $\mathcal{L}(\theta, \phi; X)$ via gradient ascent which includes the
expected reconstruction error loss and the KL-divergence term according to the distribution of the latent vectors.
When both prior and posterior are Gaussian, KL-divergence can be computed analytically:
\begin{equation}
    - D_{KL}(q(z|X;\phi)\; ||\;p(z; \theta)) = \frac{1}{2}\sum_{j=1}^J (1+\log((\sigma_j)^2)-(\mu_j)^2-(\sigma_j)^2)
\end{equation}

where J is the dimensionality of z, and $\mu_j,\sigma_j$ represent the jth element of these vectors.
The expected reconstruction error 
$\mathbb E_{q(z|X;\phi)} [\log p(X|z; \theta)]$
requires estimation by sampling, and we will use the mean-squared error between the $action_i$ from the dataset and the reconstructed
action.

Finally, when acting during evaluation or deployment, random values of z will be sampled from the multivariate normal
and passed through the decoder to produce actions.

\todo{add:}

\begin{itemize}
    \item New actor network
    \item VAE(state) + perturbation\_level $\times$ DeterministicActorNN(vae\_action, state)
    \item Train DeterministicActorNN as in the algorithm presented previously for off-policy RL.
    \item Remove randn sampling of z in deployment? It works :)
\end{itemize}

