\chapter{Introduction}
\label{sec:introduction}


Decision-making is the process of taking choices by identifying an optimal strategy
from current system states. A crucial element in the process is the performance measure used
to assess how good the choice is for the decision-maker or \textit{agent}.
In sequential decision-making problems, the most common optimization criterion is the 
expectation of the cumulative reward collected by the agent through a whole sequence
of interactions with the environment.


Reinforcement learning (RL) framework focuses on dynamic decision making in an unknown environment, by
allowing an agent to interact with it, perceiving the state of the process and acting in order to
maximize, generally, the expected cumulative reward.\\
When an agent interacts with the environment, different sources of irreducible 
randomness are introduced due to modelling errors or inherent stochasticity of the same.
Notion of risk in RL is related to the fact that even an optimal policy with respect to the 
expected cumulative reward may perform poorly in some cases.
Since maximizing the expected cumulative reward does not necessarily imply the avoidance of rare occurrences of large negative outcomes,
we need other criteria to evaluate risk. \citep{Garcia2015}.

Risk-sensitive decision-making provides a promising
approach to compute robust and safe policies, essential in safety-critical applications,
such as autonomous navigation.
However, finding computationally tractable and conceptually meaningful methodologies for such a
goal is non-trivial and still a challenge.

We propose an approach to find risk-sensitive policies by modifying the
objective function to optimize a risk metric, namely the Conditional Value at Risk (CVaR)
instead of the usual expected value.
Non-formally, CVaR of a distribution at confidence level $\alpha$ can be defined as the expected
reward of outcomes worse than the $\alpha$-tail of the distribution.
CVaR is a risk metric that has gained a lot of popularity due to its mathematical properties,
which makes its computation and optimization much easier than for other metrics  \citep{Rockafellar2000}. For example, 
it has recently been identified by \citet{Majumdar2020} as a suitable metric
for measuring risk in robotics.

The CVaR optimization algorithm that we introduce has 2 main properties that so far, we do not have
knowledge any other algorithm for CVaR optimization in the RL setting has; namely,
being an off-policy algorithm and using deterministic policies.\\
Being off-policy means that the algorithm can learn the optimal policy from data collected
from another policy. Hence, we can use a more exploratory \textit{behavior} policy to interact
with the environment and collect observations, and then use them to train
the optimal \textit{target} policy. 
Additionally to the fact of making exploration easier compared to on-policy algorithms,
being off-policy allows the possibility to learn from data generated by a conventional non-learning controller
or from a human expert.
This application is really appealing when working with risky environments and when we do not want to 
expose the learning agent to its risks at the earlier stages of the learning process.


The second property is the fact that we use a target policy which is deterministic.
Using stochastic policies is not natural in many applications and it could increase the variance
of the return \todo{\citet{Feinberg2008}/\citet{Taleghan2018}/nothing}.
When again, working in an stochastic 
environment and with the goal of finding risk-aware policies, using a policy which is stochastic 
sounds quite counterintuitive.

In this thesis, we propose a CVaR optimization approach based on a model-free, off-policy
actor-critic algorithm using deep function approximators. 
Building upon recent research in distributional RL \citep{Dabney2018b}, we design a distributional actor-critic approach
using deterministic policy gradients. The critic is trained to estimate the whole value distribution
and the actor is trained towards maximizing the tail of such distribution, namely the CVaR.

We show algorithm performance in two different settings.
First, in the most common off-policy RL setting in which the environment simulator is given and
the agent can interact with it to collect new data from time to time.
Second, and most powerful, the agent is trained completely offline by using an external
dataset and when no further interaction with the environment is allowed.
For the 2 settings, the algorithm learns risk-sensitive policies in
complex high-dimensional, continuous action spaces
such as for some modified environments from the Open AI Gym toolkit and from D4RL,
a recent suite of tasks and datasets for benchmarking progress in offline RL.








