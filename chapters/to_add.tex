
\chapter{Notes to be ad somewhere?}
\section{On-policy prediction}
\begin{equation}
    \overline{VE}(w)= \sum_{s \in \mathcal{S}} \mu(s) \big [v_\pi(s) - \hat{v}(s,w)      \big]^2
\end{equation}

$ \mu(s)$ is the state distribution.
It is not completely clear that the mean squared value error  $\overline{VE}$ is the right performance objective for RL.
The reason we are learning a value function is to find a better policy. The best value function for this purpose is not 
necessarily the best for minimizing the value error.

An ideal goal in terms of VE is find a global optimum, a weight vector $w^*$ for which
$\overline{VE}(w^*) \leq \overline{VE}(w)$ for all possible w. Reaching this goal is sometimes possible for simple
function approximators such as linear ones, but is rarely possible for complex function approximators such as artificial neural
networks.
We may seek to converge to a local optimum, a weight vector $w^*$ for which
$\overline{VE}(w^*) \leq \overline{VE}(w)$ for all w in some neighborhood of $w^*$.
Although this guarantee is only slightly reassuring, it is typically the best that can be
said for nonlinear function approximators, and often it is enough.  \citet{Sutton1998}.
Stochastic gradient descent (p201 sutton.)


When we have no access to the true value $v_\pi(s)$, but an approximation to it 
(such as for example a bootstrapping target)
$(r+\hat(v)(s'))$ or a MonteCarlo target $G_t$, we cannot perform the exact update.
If $U_t$ is an \textit{unbiased} estimate, ie $\mathbb E [U_t | S_t = s] = v_\pi(S_t)$
then w is guaranteed to converge to a local optimum under
the usual stochastic approximation conditions for decreasing $\alpha$ 

For example, suppose the states in the examples are the states generated by interaction
(or simulated interaction) with the environment using policy $\pi$. Because the true value of
a state is the expected value of the return following it, the Monte Carlo target $U_t=G_t$
is by definition an unbiased estimate of $ v_\pi(S_t)$
With this choice, the general SGD method converges to a locally optimal approximation to
$ v_\pi(S_t)$. 
If a bootstrapping estimate of $ v_\pi(S_t)$ is used as the target $U_t$ then we don't
have the same guarantees of convergence.
They all depend on the current value of the weight vector $w_t$, which implies that they will
be biased and that they will not produce a true gradient-descent method.
The equal in the following
\begin{align}
    w_{t+1}&=w_t - \frac{1}{2}\alpha\nabla [v_\pi(S_t)-\hat v(S_t, w_t)]^2 \\
    &=w_t +\alpha  [v_\pi(S_t)- \hat v(S_t, w_t)]\nabla \hat v(S_t, w_t)
\end{align}
relies on the target $v_pi$ being independent of $w_t$. This step would not be valid if a bootstrapping
estimate were used in place of $v_\pi(S_t)$.
Bootstrapping methods are not in fact instances of true gradient descent (Barnard, 1993).
They take into account the effect of changing the weight vector $w_t$ on the estimate, but
ignore its effect on the target.
They include only a part of the gradient and, accordingly, we call them \textit{semi-gradient}
methods.
Although semi-gradient (bootstrapping) methods do not converge as robustly as
gradient methods, they do converge reliably in important cases. They also provide some
important advantages such that they typically enable significantly faster learning
and they enable learning to be continual and online, without waiting for the end of an episode.

\section{On-policy control with approx}
The extension of semi-gradient prediction methods to action values i straightforward.
The update target $U_t$ cam be any approximation of $q_\pi(S_t, A_t)$ including usual backed-up values such as 
the full Monte Carlo return $G_t$ or any of the n-step Sarsa returns.
General gradient-descent update for action-value prediction is:
\begin{align}
    w_{t+1}&=w_t +\alpha  [U_t- \hat q(S_t,A_t, w_t)]\nabla \hat q(S_t,A_t, w_t)
\end{align}

for the one-step SARSA: episodic semi-gradient one-step SARSA:

\begin{align}
    w_{t+1}&=w_t +\alpha  [R_{t+1}+\gamma \hat q(S_{t+1}, A_{t+1}, w_t)- \hat q(S_t,A_t, w_t)]\nabla \hat q(S_t,A_t, w_t)
\end{align}

To form control methods, we need to couple such action-value prediction methods with
techniques for policy improvement and action selection. 

\section{Off-policy methods with approx}

The extension to function approximation is significantly harder for off-policy
learning that it is for on-policy learning.
semi-gradient algorithms do not converge as robustly as they do under on-policy 
training.
In off-policy learning we seek to learn a value function for a \textit{target policy $\pi$}
given data due to a different \textit{behavior policy $\beta$}. In prediction cases
both policies are static and given and we seek to learn either state or value functions.
In control case , action values are learned and also both policies which typically change
during learning: $\pi$ being the greedy policy wrt to estimated value function $\hat q$and 
$\beta$ being a more exploratory policy like $\epsilon-greedy$ wrt $\hat q$.

Challenge of off-policy learning can be split into two parts.
First one arises both with tabular and function approximation methods and has to do with the target of the update.
The second one only occurs with function approx methods and has to do with the fact that the distribution
of the updates is not according to the on-policy distribution, which is important for the stability
of semi-gradient methods.

